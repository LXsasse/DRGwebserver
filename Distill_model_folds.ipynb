{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7276dced",
   "metadata": {},
   "source": [
    "# Distilling multiple models into a unified framework\n",
    "\n",
    "Here we will train a distilled model on the predictions, or mean predictions from an ensemble of models trained on different folds.\n",
    "The following steps will be used:\n",
    "1. Create Training data\n",
    "\n",
    "    a. Make predictions for original sequences\n",
    "\n",
    "    b. Generate new sequences from the genome to make predictions\n",
    "\n",
    "    c. Generate new sequences with variants\n",
    "\n",
    "2. Take the mean of created data\n",
    "3. Retrain the model, use artificial sequences for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b101e",
   "metadata": {},
   "source": [
    "## 1. Create Training data from 10 fold models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3fe73",
   "metadata": {},
   "source": [
    "### a. Make predictions for original sequences with all 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aad02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drgclis = os.path.expanduser('~/Git/DRG/scripts/')\n",
    "\n",
    "def generate_ensemble_predictions(modeldict, input_file, outpath, device='cpu', drgclis_path=None):\n",
    "    \"\"\"\n",
    "    Generate predictions from multiple models and compute their mean.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    modeldict : dict\n",
    "        Dictionary mapping fold names to model paths\n",
    "    input_file : str\n",
    "        Path to the input file for predictions\n",
    "    outpath : str\n",
    "        Output directory for saving predictions\n",
    "    device : str, optional\n",
    "        Device to use for computations ('cpu' or 'cuda'). Default is 'cpu'\n",
    "    drgclis_path : str, optional\n",
    "        Path to DRG scripts. If None, uses the global drgclis variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (mean_values, columns, names) - averaged predictions and metadata\n",
    "    \"\"\"\n",
    "    if drgclis_path is None:\n",
    "        drgclis_path = drgclis\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for fold, model in modeldict.items():\n",
    "        print(f'Processing {input_file} with model {model}')\n",
    "        # Run model\n",
    "        # keep track name files in that order because it's the same as during training.\n",
    "        os.system(f'python {drgclis_path}train_models/run_cnn_model.py {input_file} None --predictnew --cnn {model}_model_params.dat device={device} --save_predictions --outname {outpath}')\n",
    "\n",
    "    # Read in predictions from individual models and create new training set out of mean\n",
    "    values_list = []\n",
    "    prevcolumns = None\n",
    "    prevnames = None\n",
    "\n",
    "    for fold, model in modeldict.items():\n",
    "        model_basename = os.path.basename(model)\n",
    "        pred_file = f'{outpath}/from{model_basename}_predictions.npz'\n",
    "        if os.path.exists(pred_file):\n",
    "            with np.load(pred_file) as data:\n",
    "                values = data['values']\n",
    "                columns = data['columns']\n",
    "                names = data['names']\n",
    "                if len(values_list) == 0:\n",
    "                    prevcolumns = columns\n",
    "                    prevnames = names\n",
    "                if np.array_equal(columns, prevcolumns) and np.array_equal(names, prevnames):\n",
    "                    values_list.append(values)\n",
    "                else:\n",
    "                    print(f'Incompatible columns or names in {pred_file}. Skipping.')\n",
    "        else:\n",
    "            print(f'Prediction file {pred_file} not found.')\n",
    "\n",
    "    if len(values_list) > 0:\n",
    "        mean_values = np.mean(values_list, axis=0)\n",
    "        return mean_values, prevcolumns, prevnames\n",
    "    else:\n",
    "        raise ValueError(\"No valid prediction files found.\")\n",
    "\n",
    "# Set up model dictionary\n",
    "modeldict = {}\n",
    "modelpath = os.path.abspath('./models/')\n",
    "modelname = 'CTCFaH3K27acaH3K36me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh'\n",
    "modelsuffix = '-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F'\n",
    "\n",
    "for f in range(10):\n",
    "    modeldict['fold'+str(f)] = f'{modelpath}/{modelname}{f}{modelsuffix}'\n",
    "\n",
    "# Set up paths and parameters\n",
    "input_path = os.path.expanduser('./')\n",
    "input_file = f'{input_path}seq2k.npz'\n",
    "device = 'cpu'\n",
    "outpath = os.path.expanduser(f'./output/')\n",
    "\n",
    "# Generate ensemble predictions\n",
    "mean_values, columns, names = generate_ensemble_predictions(\n",
    "    modeldict, input_file, outpath, device=device, drgclis_path=drgclis\n",
    ")\n",
    "\n",
    "# Save the averaged predictions\n",
    "np.savez_compressed(f'{outpath}{os.path.splitext(os.path.split(input_file)[1])[0]}.model.mean_predictions.npz', \n",
    "                   counts=mean_values, \n",
    "                   celltypes=columns, \n",
    "                   names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f96000",
   "metadata": {},
   "source": [
    "### b. Generate new sequences from the genome and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin bed original bed file\n",
    "data_path = '/home/sasse/UW/CutandRun/'\n",
    "bed_file = f'{data_path}ImmGen_ATACpeak.final.bed6' # windows for signal\n",
    "\n",
    "# Read bed and create novel 250bp region that are outside the 250 regions in the bed file\n",
    "def determine_regions_between_peaks(bed_file, signal_window = None):\n",
    "    with open(bed_file, 'r') as f:\n",
    "        peaks = [line.strip().split('\\t') for line in f.readlines()]\n",
    "    peaks = np.array(peaks)\n",
    "    if signal_window is None:\n",
    "        signal_window = int(peaks[0, 2]) - int(peaks[0, 1])  # Assuming uniform window size from the first peak\n",
    "    \n",
    "    # Convert to intervals for each chromosome separately\n",
    "    inbetween_peaks = []\n",
    "    for chr in np.unique(peaks[:, 0]):\n",
    "        chr_peaks = peaks[peaks[:, 0] == chr]\n",
    "        between_peaks = np.stack([chr_peaks[:-1, 2], chr_peaks[1:, 1]], axis=1)\n",
    "        between_peak_names = np.array([f\"{chr}:{start}-{end}\" for start, end in between_peaks])\n",
    "        between_peak_chrs = np.array([chr] * len(between_peak_names))\n",
    "        # check if between peaks > signal_window\n",
    "        valid_intervals = between_peaks[:, 1] - between_peaks[:, 0] > signal_window\n",
    "        valid_between_peaks = np.stack([between_peak_chrs[valid_intervals], between_peaks[valid_intervals]], axis=1)\n",
    "        inbetween_peaks.append(np.stack([valid_between_peaks, between_peak_names[valid_intervals]], axis=1))\n",
    "    inbetween_peaks = np.concatenate(inbetween_peaks, axis=0)\n",
    "    return inbetween_peaks\n",
    "\n",
    "region_size = 250\n",
    "sequence_length = 2000\n",
    "\n",
    "inbetween_peaks = determine_regions_between_peaks(bed_file, signal_window=region_size)\n",
    "\n",
    "# create function that splits the regions between peaks into regions of region_size, with and without overlap defined by step_size\n",
    "def split_regions_into_chunks(regions, region_size, step_size=None, can_overlap = False):\n",
    "    \"\"\"\n",
    "    Split regions into chunks of a given size with optional overlap.\n",
    "    \"\"\"\n",
    "    if step_size is None:\n",
    "        step_size = region_size\n",
    "\n",
    "    chunks = []\n",
    "    for region in regions:\n",
    "        start = region[1]\n",
    "        end = region[2]\n",
    "        ct = 0\n",
    "        while start < end:\n",
    "            chunk_end = start + region_size\n",
    "            if not can_overlap and chunk_end > end:\n",
    "                break\n",
    "            chunks.append([region[0], start, chunk_end, f'{region[3]}_{ct}'])\n",
    "            start += step_size\n",
    "            ct += 1\n",
    "            if chunk_end > end:\n",
    "                break\n",
    "    return np.array(chunks)\n",
    "\n",
    "inbetween_chunks = split_regions_into_chunks(inbetween_peaks, region_size, step_size=None, can_overlap=False)\n",
    "print(f'Created {len(inbetween_chunks)} chunks')\n",
    "\n",
    "if len(inbetween_chunks) > 1000000:\n",
    "    # Randomly downsample to 1000000 regions\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    inbetween_chunks = inbetween_chunks[np.random.choice(inbetween_chunks.shape[0], 1000000, replace=False)]\n",
    "\n",
    "# extend the chunks to sequence_length\n",
    "extension = (sequence_length - region_size) // 2\n",
    "\n",
    "# Create sequences for the chunks with the genome chromosome files\n",
    "genome_path = '/home/sasse/data/genomes/mm10/'\n",
    "import drg_tools as drg\n",
    "from drg_tools import io_utils as utils\n",
    "from drg_tools import sequence_utils as sutils\n",
    "\n",
    "seqnames, seqs = utils.extract_sequences_from_bed(bed_file, genome_path, extend_before = extension, extend_after = extension)\n",
    "\n",
    "onehot = []\n",
    "for s, seq in enumerate(seqs):\n",
    "    onehot.append(sutils.seq_onehot(seq))\n",
    "\n",
    "# Convert the list of one-hot encoded sequences to a numpy array\n",
    "onehot = np.array(onehot)\n",
    "seqnames = np.array(seqnames)\n",
    "# Print the shape of the one-hot encoded sequences\n",
    "print(onehot.shape)\n",
    "# Save the one-hot encoded sequences to a numpy file\n",
    "output_path = os.path.splitext(bed_file)[0] + 'inbetween.oh.npz'\n",
    "# Save the one-hot encoded sequences and names to a numpy file\n",
    "# call the arrays seqfeatures and genenames for compatibility with the rest of the code\n",
    "np.savez(output_path, seqfeatures = onehot, genenames = seqnames)\n",
    "print(f\"One-hot encoded sequences saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mean prediction training data for new sequences \n",
    "# Set up paths and parameters\n",
    "input_path = os.path.expanduser('./')\n",
    "input_file = output_path  # Use the one-hot encoded sequences file\n",
    "device = 'cpu'\n",
    "outpath = os.path.expanduser(f'./output/')\n",
    "\n",
    "# Generate ensemble predictions\n",
    "mean_values, columns, names = generate_ensemble_predictions(\n",
    "    modeldict, input_file, outpath, device=device, drgclis_path=drgclis\n",
    ")\n",
    "\n",
    "# Save the averaged predictions\n",
    "np.savez_compressed(f'{outpath}{os.path.splitext(os.path.split(input_file)[1])[0]}.model.mean_predictions.npz', \n",
    "                   counts=mean_values, \n",
    "                   celltypes=columns, \n",
    "                   names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240804a",
   "metadata": {},
   "source": [
    "### c. Generate new sequences with variants in original sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and parameters\n",
    "input_path = os.path.expanduser('./')\n",
    "input_file = f'{input_path}seq2k.npz'\n",
    "# Load one-hot encoded sequences\n",
    "onehot = np.load(input_file)\n",
    "# Randomly insert variants with a certain probability\n",
    "variant_probability = 0.1\n",
    "for i in range(onehot.shape[0]):\n",
    "   # Insert a variant of frequency variant_probability\n",
    "   positions = np.random.choice(0, onehot.shape[1], size=int(variant_probability * onehot.shape[1]), replace=False)\n",
    "   oh = onehot[i]\n",
    "   oh[positions, :] = 0\n",
    "   oh[positions, np.random.choice(0, onehot.shape[2], size=len(positions), replace=True)] = 1\n",
    "   onehot[i] = oh\n",
    "\n",
    "output_path = os.path.splitext(input_file)[0] + f'mutated{variant_probability}.oh.npz'\n",
    "# Save the one-hot encoded sequences and names to a numpy file\n",
    "# call the arrays seqfeatures and genenames for compatibility with the rest of the code\n",
    "np.savez(output_path, seqfeatures = onehot, genenames = seqnames)\n",
    "print(f\"One-hot encoded sequences saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed486cc0",
   "metadata": {},
   "source": [
    "## 2. Train distillation model on generated mean predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd580e",
   "metadata": {},
   "source": [
    "### a. Train on different combinations of generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc86b78f",
   "metadata": {},
   "source": [
    "### b. Assess the performance on real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaef47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
