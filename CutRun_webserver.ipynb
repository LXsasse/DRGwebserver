{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c764bb5-d5d9-4829-ac0e-20aebb4934e6",
   "metadata": {
    "id": "8c764bb5-d5d9-4829-ac0e-20aebb4934e6"
   },
   "source": [
    "# The webserver notebook\n",
    "The model can be initialized with `*_model_params.dat`. The python script will then automatically read in the matching `*_parameter.pth` file. \n",
    "\n",
    "This `*_parameter.pth` is not in the github due to size -- please access it from the google drive (`https://drive.google.com/drive/u/1/folders/1_bxdlREkogwyM5Od2x-mTDk0b3YeBj-M`) and move it to `/models/`. \n",
    "\n",
    "Similarly, the RNA-seq data (used for lineage-specific seqlet to motif matching) is not on the github -- please also access it on the google drive and move it to `/data/`. \n",
    "\n",
    "You will also need to run the bash script `wget.sh` in the `/data/mm10/` folder to download the chromosome fasta files (ex, `chr1.fa.gz`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5e05bae2-6d44-4d87-b4ef-f382fa5330b9",
   "metadata": {
    "id": "5e05bae2-6d44-4d87-b4ef-f382fa5330b9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Specify your model\n",
    "model_file='CTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F'\n",
    "model_params = './models/'+model_file+'_model_params.dat'\n",
    "\n",
    "# Specify path to drg_tools\n",
    "drgclis='~/Git/DRG/scripts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86828f-f0a0-4a13-858b-fc85f0cec2df",
   "metadata": {
    "id": "6c86828f-f0a0-4a13-858b-fc85f0cec2df"
   },
   "source": [
    "## Specify input to server\n",
    "The model takes 2000bp sequence intervals as input and predicts the ATAC-seq signal within a 250bp window in the center of that interval, as well as the signal in 1000 bp windows of 7 Cut&Run data sets. The sequence attributions show all bases and motifs that contribute to the center window only. \n",
    "\n",
    "1. Determine if you want to provide genomic location of a fasta file.\n",
    "\n",
    "    a. for general functionality, if provided a sequence, it will either be cut into overlapping 2000 bp fragments shifted by 250bp, or padded to 2000bp (not yet implemented)\n",
    "\n",
    "2. If given a location \n",
    "    \n",
    "    a. determine if you want the attribution for the signal in this region (i.e. all attributions that contribute to a window in this region, **output centric view**) or for the sequence (i.e. all attributions that that fall into this interval even if their target window is outside, **input centric view**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f549ec6d-d0b8-4d77-bbf0-4d18cce3c573",
   "metadata": {
    "id": "f549ec6d-d0b8-4d77-bbf0-4d18cce3c573"
   },
   "outputs": [],
   "source": [
    "# Specify model input\n",
    "create_sequence = True\n",
    "bed = './data/inputs/cd19test.bed'\n",
    "original_bed = bed  # Keep original bed for later use\n",
    "input_centric = False\n",
    "#sequence = './data/inputs/mm10test2000.fasta' # only used if bed given and create_sequence set to False.\n",
    "input_length = 2000 # length of the input sequence\n",
    "atac_window_size = 250\n",
    "cr_window_size = 1000\n",
    "window_size = min(atac_window_size, cr_window_size)\n",
    "step_size = window_size // 2\n",
    "background = False # If true, will create shuffled sequences for attribution background model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4c814ecd-5ef2-4fd3-818e-987ee3300e4c",
   "metadata": {
    "id": "4c814ecd-5ef2-4fd3-818e-987ee3300e4c",
    "outputId": "4682a73d-d0af-47e7-ce77-bca432d2396d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 15 sequences of length 2000 for 2000 region with input_centric=False.\n",
      "Length 145441459\n",
      "Generate seq for chr7\n",
      "Locations in chr 1\n",
      "Max sequence length Cd19 2000\n",
      "Saved as \n",
      "./data/inputs/mm10cd19test_onehot-ACGT_alignleft.npz\n",
      "Length 145441459\n",
      "Generate seq for chr7\n",
      "Locations in chr 15\n",
      "Max sequence length Cd19_cm0 2000\n",
      "Saved as \n",
      "./data/inputs/mm10cd19test_oc2000_onehot-ACGT_alignleft.npz\n"
     ]
    }
   ],
   "source": [
    "# Create necessary input files for the model\n",
    "# Note: include --'save_pos_info' so that this info can be used to create .bw and .bed files for Webserver\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "mm10='./data/mm10' # contains all chr1-19 in as chr1.fa.gz\n",
    "\n",
    "def create_centric_bed_file(bed, window_size, step_size, input_length, input_centric):\n",
    "    \"\"\"\n",
    "    Create a bed file with 2000bp sequences for all 250 bp windows in the center of the 2000bp sequence.\n",
    "    If input_centric is True, All 2000bp sequences that overlap with at least step_size with the region in the bed file are created.\n",
    "    If input_centric is False, only the 2000pb sequences whose center 250bp overlap with the region in the bed file are created.\n",
    "    The new bed file is saved with '_ic.bed' or '_oc.bed' suffix depending on the value of input_centric.\n",
    "    250bp windows are created in the center of the 2000bp sequence.\n",
    "    \"\"\"\n",
    "    newbed = bed.replace('.bed', '_ic.bed' if input_centric else '_oc.bed')\n",
    "    newfile = open(newbed, 'w')\n",
    "    for line in open(bed):\n",
    "        if not line.startswith('#'):\n",
    "            fields = line.strip().split()\n",
    "            chrom, start, end, name = fields[0], int(fields[1]), int(fields[2]), fields[3]\n",
    "            center = (start + end) // 2\n",
    "            region_length = end - start\n",
    "            newfile.write(f\"{chrom}\\t{center-step_size}\\t{center + step_size}\\t{name}_p\\n\")\n",
    "            # Need to add the input_length/2 only if input_centric is True\n",
    "            n_splits = int((region_length + input_length * int(input_centric)) / (2 * step_size)) - 1 + int((region_length + input_length * int(input_centric)) % (2 * step_size) > 0)\n",
    "            print(f'Creating {2*n_splits+1} sequences of length {input_length} for {region_length} region with input_centric={input_centric}.')\n",
    "            for i in range(int((region_length+input_length*int(input_centric))/2/step_size)-1+int((region_length+input_length*int(input_centric))%(2*step_size)>0)):\n",
    "                newfile.write(f\"{chrom}\\t{center + i*step_size}\\t{center + i*step_size + window_size}\\t{name}_cp{i}\\n\")\n",
    "                newfile.write(f\"{chrom}\\t{center -i*step_size-window_size}\\t{center - i*step_size}\\t{name}_cm{i}\\n\")\n",
    "    newfile.close()\n",
    "    return newbed\n",
    "\n",
    "def create_background_sequences(sequence_file):\n",
    "    \"\"\"\n",
    "    Create background shuffled sequences for motif detection.\n",
    "    Attribution from shuffled sequences will be used to determine significant attributions in the actual sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequence_file : str\n",
    "        Path to the fasta file containing the original sequences\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Appends shuffled sequences to the original sequence file\n",
    "    \"\"\"\n",
    "    from drg_tools.io_utils import readinfasta\n",
    "    from tangermeme.ersatz import dinucleotide_shuffle\n",
    "    from tangermeme.utils import one_hot_encode\n",
    "    import torch\n",
    "    \n",
    "    # Read fasta file and create background sequences\n",
    "    seq_names, seqs = readinfasta(sequence_file)\n",
    "    # Check if shuffled sequences already exist\n",
    "    if any(name.startswith('shuffled_') for name in seq_names):\n",
    "        print('Shuffled sequences already exist in the file. Skipping background sequence creation.')\n",
    "        return\n",
    "\n",
    "    ohseq = np.array([one_hot_encode(seq) for seq in seqs])\n",
    "    shuffled_seqs = dinucleotide_shuffle(torch.tensor(ohseq), n=1).squeeze(1).numpy()\n",
    "    rseq_names = ['shuffled_'+name for name in seq_names]\n",
    "    \n",
    "    # Convert shuffled sequences back to string format\n",
    "    shuffled_seqs = [''.join(np.array(list('ACGT'))[np.argmax(base, axis = -2)]) for base in shuffled_seqs]\n",
    "    \n",
    "    # Add the shuffled sequences to the sequence file\n",
    "    with open(sequence_file, 'a') as f:\n",
    "        for name, seq in zip(rseq_names, shuffled_seqs):\n",
    "            #print(f'{name}\\n{seq}\\n{seqs[list(seq_names).index(name.replace('shuffled_', ''))]}')\n",
    "            f.write(f'>{name}\\n'+''.join(seq) + '\\n')\n",
    "\n",
    "\n",
    "if create_sequence:\n",
    "\n",
    "    newbed = create_centric_bed_file(bed, window_size, step_size, input_length, input_centric)\n",
    "    # If input_centric is True, the new bed file will have '_ic.bed' suffix, otherwise '_oc.bed'\n",
    "\n",
    "    original_bed = bed\n",
    "    bed = newbed\n",
    "\n",
    "    prefix = os.path.split(bed)[0]\n",
    "    if prefix != '':\n",
    "        prefix = prefix+'/'\n",
    "    sequence = prefix+mm10.strip('/').split('/')[-1]+os.path.splitext(os.path.split(bed.strip('.gz'))[1])[0]+f'{input_length}.fasta'\n",
    "    original_sequence = prefix+mm10.strip('/').split('/')[-1]+os.path.splitext(os.path.split(original_bed.strip('.gz'))[1])[0]+'.fasta'\n",
    "    \n",
    "    # Create fasta form mm10 genome with bed file\n",
    "    !python {drgclis+'data_preprocessing/generate_fasta_from_bedgtf_and_genome.py'} {mm10} {original_bed} --'save_pos_info'\n",
    "    # Create one-hot encoding for input to model and attributions\n",
    "    !python {drgclis+'data_preprocessing/transform_seqtofeature.py'} {original_sequence}\n",
    "    original_sequencenpz=f'{os.path.splitext(original_sequence)[0]}_onehot-ACGT_alignleft.npz'\n",
    "\n",
    "\n",
    "# Create fasta form mm10 genome with bed file\n",
    "!python {drgclis+'data_preprocessing/generate_fasta_from_bedgtf_and_genome.py'} {mm10} {bed} --extend_to_length {input_length} --'save_pos_info'\n",
    "\n",
    "# Potentially create background shuffled sequences for motif detection\n",
    "# Attribution from shuffled sequences will be used to determine significant attributions in the actual sequences.\n",
    "# If you want to use the background sequences, set the variable 'background' to True\n",
    "\n",
    "if background:\n",
    "    create_background_sequences(sequence)\n",
    "\n",
    "# Create one-hot encoding for input to model and attributions\n",
    "!python {drgclis+'data_preprocessing/transform_seqtofeature.py'} {sequence}\n",
    "sequencenpz=f'{os.path.splitext(sequence)[0]}_onehot-ACGT_alignleft.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed995a01-8108-423b-8510-037e4fc74de5",
   "metadata": {
    "id": "ed995a01-8108-423b-8510-037e4fc74de5"
   },
   "source": [
    "## Specificy output from server\n",
    "Now, let's specify what we want to get from the model. Below are three different possible choices\n",
    "1. Return attributions for lineages and all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a48532ad-de96-4150-8a66-dccbd96d2a7e",
   "metadata": {
    "id": "a48532ad-de96-4150-8a66-dccbd96d2a7e"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'all' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'all' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = True # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = True # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144abb3",
   "metadata": {},
   "source": [
    "1.b. All attributions of lineages for ATAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "77ea0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'all' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'ATAC' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = True # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = True # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90534f-0ff5-488b-b1a8-587066867510",
   "metadata": {
    "id": "9f90534f-0ff5-488b-b1a8-587066867510"
   },
   "source": [
    "2. Return attributions for only one cell type and one modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9d6a576-802f-4951-bb6f-b388a733ee97",
   "metadata": {
    "id": "c9d6a576-802f-4951-bb6f-b388a733ee97"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'B.Fem.Sp' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'ATAC' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = False # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = True # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190b255-918c-4e21-9365-d7f967c60d7a",
   "metadata": {
    "id": "0190b255-918c-4e21-9365-d7f967c60d7a"
   },
   "source": [
    "3. Compare B-cell attributions across modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "584343aa-71fc-43e8-a76d-838d5aa1b013",
   "metadata": {
    "id": "584343aa-71fc-43e8-a76d-838d5aa1b013"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp,B.GC.CC.Sp,B.MZ.Sp,B.PB.Sp,B.PC.BM,B.PC.Sp,B.Sp,B.T1.Sp,B.T2.Sp,B.T3.Sp,B.mem.Sp,B1b.PC' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'all' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = True # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = False # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "413146d2-eded-4d61-94f4-5b0992856c2e",
   "metadata": {
    "id": "413146d2-eded-4d61-94f4-5b0992856c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATAC', 'CTCF', 'H33', 'H3K4me1', 'H3K4me3', 'H3K27me3', 'H3K27ac', 'H3K36me3'] ['B.Fem.Sp', 'B.Fo.Sp', 'B.FrE.BM', 'B.GC.CB.Sp', 'B.GC.CC.Sp', 'B.MZ.Sp', 'B.PB.Sp', 'B.PC.BM', 'B.PC.Sp', 'B.Sp', 'B.T1.Sp', 'B.T2.Sp', 'B.T3.Sp', 'B.mem.Sp', 'B1b.PC']\n",
      "Selected tracks: --select_tracks B.Fem.Sp.ATAC,B.Fo.Sp.ATAC,B.FrE.BM.ATAC,B.GC.CB.Sp.ATAC,B.GC.CC.Sp.ATAC,B.MZ.Sp.ATAC,B.PB.Sp.ATAC,B.PC.BM.ATAC,B.PC.Sp.ATAC,B.Sp.ATAC,B.T1.Sp.ATAC,B.T2.Sp.ATAC,B.T3.Sp.ATAC,B.mem.Sp.ATAC,B1b.PC.ATAC,B.Fem.Sp.CTCF,B.Fo.Sp.CTCF,B.FrE.BM.CTCF,B.GC.CB.Sp.CTCF,B.GC.CC.Sp.CTCF,B.MZ.Sp.CTCF,B.PB.Sp.CTCF,B.PC.BM.CTCF,B.PC.Sp.CTCF,B.Sp.CTCF,B.T1.Sp.CTCF,B.T2.Sp.CTCF,B.T3.Sp.CTCF,B.mem.Sp.CTCF,B1b.PC.CTCF,B.Fem.Sp.H33,B.Fo.Sp.H33,B.FrE.BM.H33,B.GC.CB.Sp.H33,B.GC.CC.Sp.H33,B.MZ.Sp.H33,B.PB.Sp.H33,B.PC.BM.H33,B.PC.Sp.H33,B.Sp.H33,B.T1.Sp.H33,B.T2.Sp.H33,B.T3.Sp.H33,B.mem.Sp.H33,B1b.PC.H33,B.Fem.Sp.H3K4me1,B.Fo.Sp.H3K4me1,B.FrE.BM.H3K4me1,B.GC.CB.Sp.H3K4me1,B.GC.CC.Sp.H3K4me1,B.MZ.Sp.H3K4me1,B.PB.Sp.H3K4me1,B.PC.BM.H3K4me1,B.PC.Sp.H3K4me1,B.Sp.H3K4me1,B.T1.Sp.H3K4me1,B.T2.Sp.H3K4me1,B.T3.Sp.H3K4me1,B.mem.Sp.H3K4me1,B1b.PC.H3K4me1,B.Fem.Sp.H3K4me3,B.Fo.Sp.H3K4me3,B.FrE.BM.H3K4me3,B.GC.CB.Sp.H3K4me3,B.GC.CC.Sp.H3K4me3,B.MZ.Sp.H3K4me3,B.PB.Sp.H3K4me3,B.PC.BM.H3K4me3,B.PC.Sp.H3K4me3,B.Sp.H3K4me3,B.T1.Sp.H3K4me3,B.T2.Sp.H3K4me3,B.T3.Sp.H3K4me3,B.mem.Sp.H3K4me3,B1b.PC.H3K4me3,B.Fem.Sp.H3K27me3,B.Fo.Sp.H3K27me3,B.FrE.BM.H3K27me3,B.GC.CB.Sp.H3K27me3,B.GC.CC.Sp.H3K27me3,B.MZ.Sp.H3K27me3,B.PB.Sp.H3K27me3,B.PC.BM.H3K27me3,B.PC.Sp.H3K27me3,B.Sp.H3K27me3,B.T1.Sp.H3K27me3,B.T2.Sp.H3K27me3,B.T3.Sp.H3K27me3,B.mem.Sp.H3K27me3,B1b.PC.H3K27me3,B.Fem.Sp.H3K27ac,B.Fo.Sp.H3K27ac,B.FrE.BM.H3K27ac,B.GC.CB.Sp.H3K27ac,B.GC.CC.Sp.H3K27ac,B.MZ.Sp.H3K27ac,B.PB.Sp.H3K27ac,B.PC.BM.H3K27ac,B.PC.Sp.H3K27ac,B.Sp.H3K27ac,B.T1.Sp.H3K27ac,B.T2.Sp.H3K27ac,B.T3.Sp.H3K27ac,B.mem.Sp.H3K27ac,B1b.PC.H3K27ac,B.Fem.Sp.H3K36me3,B.Fo.Sp.H3K36me3,B.FrE.BM.H3K36me3,B.GC.CB.Sp.H3K36me3,B.GC.CC.Sp.H3K36me3,B.MZ.Sp.H3K36me3,B.PB.Sp.H3K36me3,B.PC.BM.H3K36me3,B.PC.Sp.H3K36me3,B.Sp.H3K36me3,B.T1.Sp.H3K36me3,B.T2.Sp.H3K36me3,B.T3.Sp.H3K36me3,B.mem.Sp.H3K36me3,B1b.PC.H3K36me3\n"
     ]
    }
   ],
   "source": [
    "# Use the given input arguments to generate inputs for the model\n",
    "import pandas as pd\n",
    "\n",
    "# Load master file for lineages for selection and potential lineage summary\n",
    "lineage_file = './data/CutandRun_and_ATAC.lineages.txt'\n",
    "lineage_frame = pd.read_table(lineage_file, header = None, names = ['cell_type', 'lineage'])\n",
    "data_modalities = 'ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3'.split(',')\n",
    "outdir='./results/' # directory to save files to\n",
    "\n",
    "# Select tracks that will be returned\n",
    "if cell_types == 'all':\n",
    "    cell_types = lineage_frame['cell_type']\n",
    "elif ',' in cell_types:\n",
    "    cell_types = cell_types.split(',')\n",
    "else: # for single cell type\n",
    "    cell_types = [cell_types]\n",
    "\n",
    "if modalities == 'all':\n",
    "    modalities = data_modalities\n",
    "elif ',' in modalities:\n",
    "    modalities = modalities.split(',')\n",
    "else: # for single modality\n",
    "    modalities = [modalities]\n",
    "print(modalities, cell_types)\n",
    "tracks = '--select_tracks '\n",
    "for modal in modalities:\n",
    "    for cell_type in cell_types:\n",
    "        tracks += f'{cell_type}.{modal},'\n",
    "tracks = tracks.strip(',')\n",
    "print(f\"Selected tracks: {tracks}\")\n",
    "\n",
    "# Average over cell lineages\n",
    "mean_lineage = ''\n",
    "mean_lineage_file = './data/CutandRun_and_ATAC.lineages.modsep.tsv'\n",
    "if lineages:\n",
    "    mean_lineage = f'--average_outclasses {mean_lineage_file}'\n",
    "\n",
    "# Define sequence attribuitons that should be returned\n",
    "seq_atts = ''\n",
    "if return_attribution:\n",
    "    seq_atts = f'--sequence_attributions {attribution_type} all'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89bcb6-279a-43a1-b398-e17c44157f0d",
   "metadata": {
    "id": "ea89bcb6-279a-43a1-b398-e17c44157f0d"
   },
   "source": [
    "## Run model to compute predictions (and attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "193994e3-086e-4342-b831-91adc56758ff",
   "metadata": {
    "id": "193994e3-086e-4342-b831-91adc56758ff",
    "outputId": "ff83473f-5550-4062-875e-aac827bb3da9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ~/Git/DRG/scripts/train_models/run_cnn_model.py ./data/inputs/mm10cd19test_oc2000_onehot-ACGT_alignleft.npz None --predictnew --cnn ./models/CTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_model_params.dat device=cpu --load_output_track_names ./data/CTCF_tracks.txt,./data/H3K27ac_tracks.txt,./data/H3K36me3_tracks.txt,./data/H3K4me3_tracks.txt,./data/H33_tracks.txt,./data/H3K27me3_tracks.txt,./data/H3K4me1_tracks.txt,./data/ATAC_tracks.txt --select_tracks B.Fem.Sp.ATAC,B.Fo.Sp.ATAC,B.FrE.BM.ATAC,B.GC.CB.Sp.ATAC,B.GC.CC.Sp.ATAC,B.MZ.Sp.ATAC,B.PB.Sp.ATAC,B.PC.BM.ATAC,B.PC.Sp.ATAC,B.Sp.ATAC,B.T1.Sp.ATAC,B.T2.Sp.ATAC,B.T3.Sp.ATAC,B.mem.Sp.ATAC,B1b.PC.ATAC,B.Fem.Sp.CTCF,B.Fo.Sp.CTCF,B.FrE.BM.CTCF,B.GC.CB.Sp.CTCF,B.GC.CC.Sp.CTCF,B.MZ.Sp.CTCF,B.PB.Sp.CTCF,B.PC.BM.CTCF,B.PC.Sp.CTCF,B.Sp.CTCF,B.T1.Sp.CTCF,B.T2.Sp.CTCF,B.T3.Sp.CTCF,B.mem.Sp.CTCF,B1b.PC.CTCF,B.Fem.Sp.H33,B.Fo.Sp.H33,B.FrE.BM.H33,B.GC.CB.Sp.H33,B.GC.CC.Sp.H33,B.MZ.Sp.H33,B.PB.Sp.H33,B.PC.BM.H33,B.PC.Sp.H33,B.Sp.H33,B.T1.Sp.H33,B.T2.Sp.H33,B.T3.Sp.H33,B.mem.Sp.H33,B1b.PC.H33,B.Fem.Sp.H3K4me1,B.Fo.Sp.H3K4me1,B.FrE.BM.H3K4me1,B.GC.CB.Sp.H3K4me1,B.GC.CC.Sp.H3K4me1,B.MZ.Sp.H3K4me1,B.PB.Sp.H3K4me1,B.PC.BM.H3K4me1,B.PC.Sp.H3K4me1,B.Sp.H3K4me1,B.T1.Sp.H3K4me1,B.T2.Sp.H3K4me1,B.T3.Sp.H3K4me1,B.mem.Sp.H3K4me1,B1b.PC.H3K4me1,B.Fem.Sp.H3K4me3,B.Fo.Sp.H3K4me3,B.FrE.BM.H3K4me3,B.GC.CB.Sp.H3K4me3,B.GC.CC.Sp.H3K4me3,B.MZ.Sp.H3K4me3,B.PB.Sp.H3K4me3,B.PC.BM.H3K4me3,B.PC.Sp.H3K4me3,B.Sp.H3K4me3,B.T1.Sp.H3K4me3,B.T2.Sp.H3K4me3,B.T3.Sp.H3K4me3,B.mem.Sp.H3K4me3,B1b.PC.H3K4me3,B.Fem.Sp.H3K27me3,B.Fo.Sp.H3K27me3,B.FrE.BM.H3K27me3,B.GC.CB.Sp.H3K27me3,B.GC.CC.Sp.H3K27me3,B.MZ.Sp.H3K27me3,B.PB.Sp.H3K27me3,B.PC.BM.H3K27me3,B.PC.Sp.H3K27me3,B.Sp.H3K27me3,B.T1.Sp.H3K27me3,B.T2.Sp.H3K27me3,B.T3.Sp.H3K27me3,B.mem.Sp.H3K27me3,B1b.PC.H3K27me3,B.Fem.Sp.H3K27ac,B.Fo.Sp.H3K27ac,B.FrE.BM.H3K27ac,B.GC.CB.Sp.H3K27ac,B.GC.CC.Sp.H3K27ac,B.MZ.Sp.H3K27ac,B.PB.Sp.H3K27ac,B.PC.BM.H3K27ac,B.PC.Sp.H3K27ac,B.Sp.H3K27ac,B.T1.Sp.H3K27ac,B.T2.Sp.H3K27ac,B.T3.Sp.H3K27ac,B.mem.Sp.H3K27ac,B1b.PC.H3K27ac,B.Fem.Sp.H3K36me3,B.Fo.Sp.H3K36me3,B.FrE.BM.H3K36me3,B.GC.CB.Sp.H3K36me3,B.GC.CC.Sp.H3K36me3,B.MZ.Sp.H3K36me3,B.PB.Sp.H3K36me3,B.PC.BM.H3K36me3,B.PC.Sp.H3K36me3,B.Sp.H3K36me3,B.T1.Sp.H3K36me3,B.T2.Sp.H3K36me3,B.T3.Sp.H3K36me3,B.mem.Sp.H3K36me3,B1b.PC.H3K36me3 --save_predictions --sequence_attributions grad all --average_outclasses ./data/CutandRun_and_ATAC.lineages.modsep.tsv --outname ./results/\n",
      "None is not a valid file.\n",
      "Input shapes X: (15, 4, 2000)\n",
      "Device cpu\n",
      "In features 4 2000\n",
      "Convolutions 512 2000\n",
      "Pooling 512 200\n",
      "Convolution after attention 512 8\n",
      "Before FCL 4096\n",
      "outclasses [67, 93, 77, 81, 56, 94, 80, 81]\n",
      "Loaded convolutions.conv1d.weight with convolutions.conv1d.weight\n",
      "Loaded convolutions.conv1d.bias with convolutions.conv1d.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.weight with trconvolution_layers.convlayers.Bnorm0.weight\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.bias with trconvolution_layers.convlayers.Bnorm0.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.running_mean with trconvolution_layers.convlayers.Bnorm0.running_mean\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.running_var with trconvolution_layers.convlayers.Bnorm0.running_var\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.num_batches_tracked with trconvolution_layers.convlayers.Bnorm0.num_batches_tracked\n",
      "Loaded trconvolution_layers.convlayers.Conv0.conv1d.weight with trconvolution_layers.convlayers.Conv0.conv1d.weight\n",
      "Loaded trconvolution_layers.convlayers.Conv0.conv1d.bias with trconvolution_layers.convlayers.Conv0.conv1d.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.weight with trconvolution_layers.convlayers.Bnorm1.weight\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.bias with trconvolution_layers.convlayers.Bnorm1.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.running_mean with trconvolution_layers.convlayers.Bnorm1.running_mean\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.running_var with trconvolution_layers.convlayers.Bnorm1.running_var\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.num_batches_tracked with trconvolution_layers.convlayers.Bnorm1.num_batches_tracked\n",
      "Loaded trconvolution_layers.convlayers.Conv1.conv1d.weight with trconvolution_layers.convlayers.Conv1.conv1d.weight\n",
      "Loaded trconvolution_layers.convlayers.Conv1.conv1d.bias with trconvolution_layers.convlayers.Conv1.conv1d.bias\n",
      "Loaded nfcs.0.nfcs.EmbeddtoFully.weight with nfcs.0.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.0.nfcs.EmbeddtoFully.bias with nfcs.0.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected0.weight with nfcs.0.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected0.bias with nfcs.0.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected1.weight with nfcs.0.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected1.bias with nfcs.0.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected2.weight with nfcs.0.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected2.bias with nfcs.0.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.1.nfcs.EmbeddtoFully.weight with nfcs.1.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.1.nfcs.EmbeddtoFully.bias with nfcs.1.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected0.weight with nfcs.1.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected0.bias with nfcs.1.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected1.weight with nfcs.1.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected1.bias with nfcs.1.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected2.weight with nfcs.1.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected2.bias with nfcs.1.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.2.nfcs.EmbeddtoFully.weight with nfcs.2.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.2.nfcs.EmbeddtoFully.bias with nfcs.2.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected0.weight with nfcs.2.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected0.bias with nfcs.2.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected1.weight with nfcs.2.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected1.bias with nfcs.2.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected2.weight with nfcs.2.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected2.bias with nfcs.2.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.3.nfcs.EmbeddtoFully.weight with nfcs.3.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.3.nfcs.EmbeddtoFully.bias with nfcs.3.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected0.weight with nfcs.3.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected0.bias with nfcs.3.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected1.weight with nfcs.3.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected1.bias with nfcs.3.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected2.weight with nfcs.3.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected2.bias with nfcs.3.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.4.nfcs.EmbeddtoFully.weight with nfcs.4.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.4.nfcs.EmbeddtoFully.bias with nfcs.4.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected0.weight with nfcs.4.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected0.bias with nfcs.4.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected1.weight with nfcs.4.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected1.bias with nfcs.4.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected2.weight with nfcs.4.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected2.bias with nfcs.4.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.5.nfcs.EmbeddtoFully.weight with nfcs.5.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.5.nfcs.EmbeddtoFully.bias with nfcs.5.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected0.weight with nfcs.5.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected0.bias with nfcs.5.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected1.weight with nfcs.5.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected1.bias with nfcs.5.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected2.weight with nfcs.5.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected2.bias with nfcs.5.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.6.nfcs.EmbeddtoFully.weight with nfcs.6.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.6.nfcs.EmbeddtoFully.bias with nfcs.6.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected0.weight with nfcs.6.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected0.bias with nfcs.6.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected1.weight with nfcs.6.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected1.bias with nfcs.6.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected2.weight with nfcs.6.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected2.bias with nfcs.6.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.7.nfcs.EmbeddtoFully.weight with nfcs.7.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.7.nfcs.EmbeddtoFully.bias with nfcs.7.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected0.weight with nfcs.7.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected0.bias with nfcs.7.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected1.weight with nfcs.7.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected1.bias with nfcs.7.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected2.weight with nfcs.7.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected2.bias with nfcs.7.nfcs.Fullyconnected2.bias\n",
      "Loaded classifier.0.classifier.Linear.weight with classifier.0.classifier.Linear.weight\n",
      "Loaded classifier.0.classifier.Linear.bias with classifier.0.classifier.Linear.bias\n",
      "Loaded classifier.1.classifier.Linear.weight with classifier.1.classifier.Linear.weight\n",
      "Loaded classifier.1.classifier.Linear.bias with classifier.1.classifier.Linear.bias\n",
      "Loaded classifier.2.classifier.Linear.weight with classifier.2.classifier.Linear.weight\n",
      "Loaded classifier.2.classifier.Linear.bias with classifier.2.classifier.Linear.bias\n",
      "Loaded classifier.3.classifier.Linear.weight with classifier.3.classifier.Linear.weight\n",
      "Loaded classifier.3.classifier.Linear.bias with classifier.3.classifier.Linear.bias\n",
      "Loaded classifier.4.classifier.Linear.weight with classifier.4.classifier.Linear.weight\n",
      "Loaded classifier.4.classifier.Linear.bias with classifier.4.classifier.Linear.bias\n",
      "Loaded classifier.5.classifier.Linear.weight with classifier.5.classifier.Linear.weight\n",
      "Loaded classifier.5.classifier.Linear.bias with classifier.5.classifier.Linear.bias\n",
      "Loaded classifier.6.classifier.Linear.weight with classifier.6.classifier.Linear.weight\n",
      "Loaded classifier.6.classifier.Linear.bias with classifier.6.classifier.Linear.bias\n",
      "Loaded classifier.7.classifier.Linear.weight with classifier.7.classifier.Linear.weight\n",
      "Loaded classifier.7.classifier.Linear.bias with classifier.7.classifier.Linear.bias\n",
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F\n",
      "SAVED ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_pred.npz (15, 48)\n",
      "TISM time for (15, 8, 4, 2000) 8 5.983091354370117\n",
      "Attributions saved as ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradall.npz\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "## determine if you have a gpu for computations\n",
    "device='cpu'\n",
    "\n",
    "# keep track name files in that order because it's the same as during training.\n",
    "track_names='--load_output_track_names ./data/CTCF_tracks.txt,./data/H3K27ac_tracks.txt,./data/H3K36me3_tracks.txt,./data/H3K4me3_tracks.txt,./data/H33_tracks.txt,./data/H3K27me3_tracks.txt,./data/H3K4me1_tracks.txt,./data/ATAC_tracks.txt'\n",
    "print(f\"python {drgclis+'train_models/run_cnn_model.py'} {sequencenpz} None --predictnew --cnn {model_params} {'device='+device} {track_names} {tracks} --save_predictions {seq_atts} {mean_lineage} --outname {outdir}\")\n",
    "!python {drgclis+'train_models/run_cnn_model.py'} {sequencenpz} None --predictnew --cnn {model_params} {'device='+device} {track_names} {tracks} --save_predictions {seq_atts} {mean_lineage} --outname {outdir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651baa7",
   "metadata": {},
   "source": [
    "### In case of an output centric view sum all the individual attributions over the original window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d15fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining attributions with method center_weighted\n",
      "Input centric: False\n",
      "Attributions in original file: (15,), (15, 8, 4, 2000), (8,)\n",
      "Attribution average shape: (1, 8, 4, 2000), names: ['Cd19']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_attributions_over_windows(original_bed_path, att_names, att_values, att_exp, input_length, input_centric, \n",
    "                                      step_size, combination_method='average', combination_window_size=250):\n",
    "    \"\"\"\n",
    "    #TODO: \n",
    "    # Implement weighted and max combination methods.\n",
    "    # Implement correct averaging, so that regions with more windows are weighted less.\n",
    "\n",
    "    Average attributions over all windows in the original sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_bed_path : str\n",
    "        Path to the original bed file with regions to average over\n",
    "    att_names : np.array\n",
    "        Array of individual attribution names, need to be in the format 'name_p', 'name_cp0', 'name_cm0', etc.\n",
    "    att_values : np.array\n",
    "        Array of individual attribution values\n",
    "    att_exp : np.array\n",
    "        Array of experiments, only required when combination_method is 'weighted'\n",
    "    input_length : int\n",
    "        Length of the attributions \n",
    "    input_centric : bool\n",
    "        Whether input centric view is used\n",
    "    step_size : int\n",
    "        Step size for sliding windows\n",
    "    combination_method : str, optional\n",
    "        Method to combine attributions, can be 'average', 'weighted', or 'max'. Default is 'average', and only 'average' is implemented.\n",
    "    combination_window_size : int, or dict, optional\n",
    "        Size of the window for combination, only used if combination_method is 'weighted'. Default is 250 for all modalities.\n",
    "        Can be a single integer or a dictionary with modality names as keys and window sizes as values.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (attribution_average, attribution_names, att_exp)\n",
    "        - attribution_average: averaged attributions array\n",
    "        - attribution_names: names array for averaged attributions\n",
    "        - att_exp: experiments array (unchanged)\n",
    "    \"\"\"\n",
    "    print(f\"Combining attributions with method {combination_method}\")\n",
    "    print(f\"Input centric: {input_centric}\")\n",
    "    print(f'Attributions in original file: {att_names.shape}, {att_values.shape}, {att_exp.shape}')\n",
    "    \n",
    "    # Read in original bed file\n",
    "    bed_obj = open(original_bed_path, 'r')\n",
    "    \n",
    "    attribution_average = []\n",
    "    attribution_names = []\n",
    "\n",
    "    # get the modalities for attributions\n",
    "    modalities = np.array([ae.rsplit('.',1)[1] for ae in att_exp])\n",
    "    unique_modalities = np.unique(modalities)\n",
    "    weights = {modality: np.ones(input_length) for modality in modalities}\n",
    "    # Check if combination_method is valid\n",
    "    if combination_method not in ['average', 'linear_weighted', 'center_weighted']:\n",
    "        raise ValueError(f\"Invalid combination method: {combination_method}. Choose from 'average', 'linear_weighted', or 'center_weighted'.\")\n",
    "\n",
    "    if 'weighted' in combination_method:\n",
    "        # If combination method is weighted, we need to create a weight tensor\n",
    "        if isinstance(combination_window_size, dict):\n",
    "            # If combination_window_size is a dictionary, use the values for each modality\n",
    "            for modality, size in combination_window_size.items():\n",
    "                if modality in modalities:\n",
    "                    # create weights for an array of size input length with assigning the window size in the center the highest value\n",
    "                    steps = (input_length+size)//2//size + int((input_length-size)//2 % size > 0)\n",
    "                    # create window weights\n",
    "                    if combination_method == 'linear_weighted':\n",
    "                        win_weights = [1/(j+1) for j in range(steps)]\n",
    "                    elif combination_method == 'center_weighted':\n",
    "                        win_weights = [1.0] + [0.0 for j in range(steps-1)]\n",
    "                    wmo = np.concatenate([np.ones(size) * win_weights[j] for j in range(steps)])\n",
    "                    wmo = wmo[(wmo.shape[0]- input_length//2)//2:wmo.shape[0]-(wmo.shape[0]- input_length//2)//2]  # ensure the weights are of length input_length\n",
    "                    weights[modality] = np.append(wmo[::-1], wmo)\n",
    "        else:\n",
    "            # If combination_window_size is a single integer, use it for all modalities\n",
    "            for modality in unique_modalities:\n",
    "                # create weights for an array of size input length with assigning the window size in the center the highest value\n",
    "                steps = (input_length+combination_window_size)//2//combination_window_size + int((input_length-combination_window_size)//2 % combination_window_size > 0)\n",
    "                # create window weights\n",
    "                if combination_method == 'linear_weighted':\n",
    "                    win_weights = [1/(j+1) for j in range(steps)]\n",
    "                elif combination_method == 'center_weighted':\n",
    "                    win_weights = [1.0] + [0.0 for j in range(steps-1)]\n",
    "                wmo = np.concatenate([np.ones(combination_window_size) * win_weights[j] for j in range(steps)])\n",
    "                wmo = wmo[(wmo.shape[0]- input_length//2)//2:wmo.shape[0]-(wmo.shape[0]- input_length//2)//2]\n",
    "                # ensure the weights are of length input_length\n",
    "                wmo = np.append(wmo[::-1], wmo)\n",
    "                weights[modality] = wmo\n",
    "\n",
    "    # Average over all windows in the original sequence\n",
    "    for region in bed_obj:\n",
    "        if not region.startswith('#'):\n",
    "            fields = region.strip().split()\n",
    "            chrom, start, end, name = fields[0], int(fields[1]), int(fields[2]), fields[3]\n",
    "            region_length = end - start\n",
    "\n",
    "            # Get the attributions for the current region\n",
    "            att_average = att_values[att_names == name + '_p']\n",
    "            att_counts = np.zeros_like(att_average)\n",
    "\n",
    "            n_splits = int((end-start+input_length*int(input_centric))/2/step_size)-1+int((end-start+input_length*int(input_centric))%(2*step_size)>0)\n",
    "            n_ = n_splits * 2 + 1\n",
    "            att_average = att_average[..., max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)]\n",
    "            \n",
    "            for modality in unique_modalities:\n",
    "                modmask = modalities == modality\n",
    "                att_average[:, modmask, :, max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)] *= weights[modality][max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)]\n",
    "                att_counts[:, modmask, :, max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)] += weights[modality][max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)]\n",
    "            \n",
    "            for i in range(n_splits):\n",
    "                att_namep = f'{name}_cp{i}'\n",
    "                att_namem = f'{name}_cm{i}'\n",
    "                if att_namep in att_names:\n",
    "                    idx = np.where(att_names == att_namep)[0]\n",
    "                    # Add the attributions to the original sequence\n",
    "                    # Calculate where the start would be given region_length and input_length\n",
    "                    start_at = region_length//2 - (input_length//2 - step_size*(i+1))\n",
    "                    end_at = input_length//2 - (region_length//2) - step_size*(i+1)\n",
    "                    # If region_length//2 + step_size < input_length//2, we start 0 for assignment\n",
    "                    # Else if region_length//2 + step_size >= input_length//2, we start at region_length//2 + step_size - input_length//2\n",
    "                    start_average = max(0, start_at) # if start_at < 0, we start at 0\n",
    "                    # End average is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_average = min(input_length+start_average, region_length)\n",
    "                    # if input_length//2 > region_length//2+step_size*(i+1), not the entire region is mapped\n",
    "                    start_attr = max(0, end_at)\n",
    "                    # End attribute is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_attr = min(input_length, min(start_attr,end_at) + region_length)\n",
    "                    #print(np.corrcoef(att_average[..., start_average: end_average].flatten(), att_values[idx][..., start_attr: end_attr].flatten()))\n",
    "                    #print(np.corrcoef(att_average[..., start_average+1: end_average].flatten(), att_values[idx][..., start_attr: end_attr-1].flatten()))\n",
    "                    for modality in unique_modalities:\n",
    "                        modmask = modalities == modality\n",
    "                        att_average[:, modmask, :, start_average: end_average] += att_values[idx, modmask,..., start_attr: end_attr] * weights[modality][start_attr: end_attr]\n",
    "                        att_counts[:, modmask, :, start_average: end_average] += weights[modality][start_attr: end_attr]\n",
    "                    #print(start_average, end_average, start_attr, end_attr, att_values[idx].shape)\n",
    "                if att_namem in att_names:\n",
    "                    idx = np.where(att_names == att_namem)[0]\n",
    "                    # Now do the indexing to the left side of the region\n",
    "                    # Calculate where the start would be given region_length and input_length\n",
    "                    start_at = region_length//2 - (input_length//2 + step_size*(i+1))\n",
    "                    end_at = input_length//2 - (region_length//2) - step_size*(i+1)\n",
    "                    # If region_length//2 + step_size < input_length//2, we start 0 for assignment\n",
    "                    # Else if region_length//2 + step_size >= input_length//2, we start at region_length//2 + step_size - input_length//2\n",
    "                    start_average = max(0, start_at)\n",
    "                    # End average is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_average = min(input_length+start_average, region_length + min(0, input_length//2-region_length//2-step_size*(i+1)))\n",
    "                    # if input_length//2 > region_length//2+step_size*(i+1), not the entire region is mapped\n",
    "                    start_attr = max(0, input_length//2 - (region_length//2) + step_size*(i+1))\n",
    "                    # End attribute is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_attr = min(input_length, start_attr + region_length)\n",
    "                    # Add the attributions to the original sequence\n",
    "                    for modality in unique_modalities:\n",
    "                        modmask = modalities == modality\n",
    "                        att_average[:, modmask, :, start_average: end_average] += att_values[idx, modmask,..., start_attr: end_attr] * weights[modality][start_attr: end_attr]\n",
    "                        att_counts[:, modmask, :, start_average: end_average] += weights[modality][start_attr: end_attr]\n",
    "                    #print(np.corrcoef(att_average[..., start_average: end_average].flatten(), att_values[idx][..., start_attr: end_attr].flatten()))\n",
    "                    #print(start_average, end_average, start_attr, end_attr)\n",
    "            attribution_average.append(att_average/att_counts.max(axis=-1, keepdims=True))  # Normalize by the maximum count to avoid division by zero\n",
    "            attribution_names.append(name)\n",
    "    \n",
    "    bed_obj.close()\n",
    "    \n",
    "    # Concatenate and convert to arrays\n",
    "    attribution_average = np.concatenate(attribution_average, axis = 0)\n",
    "    attribution_names = np.array(attribution_names)\n",
    "    print(f\"Attribution average shape: {attribution_average.shape}, names: {attribution_names}\")\n",
    "    \n",
    "    return attribution_average, attribution_names, att_exp\n",
    "\n",
    "\n",
    "# Combination methods: Average, Weighted, Max \n",
    "# Weighted would weight each window from the center differently with 1/((j+1)*n_), giving higher weights to intervals that are close to the window\n",
    "# We would multiply this weight tensor beforehand with the attribution values.\n",
    "# Need to generate two weighted tensors, one with atac_window_size and one with cr_window_size\n",
    "combination_method = 'center_weighted'  # 'average', 'linear_weighted', 'center_weighted''\n",
    "\n",
    "# Create output centric view\n",
    "# Note: this is only possible if the sequence was created from a bed file.\n",
    "# Read in attributions\n",
    "attarrays=outdir+'from'+model_file+'_'+attribution_type+'all.npz'\n",
    "attributions = np.load(attarrays, allow_pickle=True)\n",
    "att_names = attributions['names']\n",
    "att_values = attributions['values']\n",
    "att_exp = attributions['experiments']\n",
    "\n",
    "# Call the function to average attributions\n",
    "attribution_average, attribution_names, att_exp = average_attributions_over_windows(\n",
    "    original_bed, att_names, att_values, att_exp, input_length, input_centric, step_size, combination_method,\n",
    "    combination_window_size={'ATAC': 250, 'CTCF': 1000, 'H33': 1000, 'H3K4me1': 1000, 'H3K4me3': 1000,\n",
    "                             'H3K27me3': 1000, 'H3K27ac': 1000, 'H3K36me3': 1000}\n",
    ")\n",
    "\n",
    "# Save the average attributions\n",
    "attarrays = attarrays.replace('.npz', 'avg.npz')\n",
    "np.savez_compressed(attarrays, values=attribution_average, names=attribution_names, experiments=att_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140900ce-228a-4967-8812-72fbe92f395b",
   "metadata": {
    "id": "140900ce-228a-4967-8812-72fbe92f395b"
   },
   "source": [
    "## Visualize all selected attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714252ce-288d-4807-8277-2cb6b520eb85",
   "metadata": {
    "id": "714252ce-288d-4807-8277-2cb6b520eb85",
    "outputId": "a1a6b60b-65d7-4075-9758-56d434e305e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cd19\n",
      "Cd19 Cd19 ['B.ATAC' 'B.CTCF' 'B.H33' 'B.H3K27ac' 'B.H3K27me3' 'B.H3K36me3'\n",
      " 'B.H3K4me1' 'B.H3K4me3']\n",
      "(8, 2000, 4)\n",
      "New range [np.int64(168), np.int64(1698)]\n",
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavg_Cd19_all.jpg\n"
     ]
    }
   ],
   "source": [
    "# Visualize attributions\n",
    "# Some visualization choices\n",
    "remove_edges_with_less = 0.25 # removes left and right parts of the attribution map that don't have attribtuions larger than this fraction of the maximum in the sequence\n",
    "dpi=100 # resolution\n",
    "\n",
    "from drg_tools.io_utils import readinfasta\n",
    "#attarrays=outdir+'from'+model_file+'_'+attribution_type+'all.npz'\n",
    "if create_sequence:\n",
    "    seq_names, seqs = readinfasta(original_sequence)\n",
    "    sequencenpz = original_sequencenpz\n",
    "else:\n",
    "    seq_names, seqs = readinfasta(sequence)\n",
    "\n",
    "jpgs = []\n",
    "if plotpermodality:\n",
    "    for modal in modalities:\n",
    "        for s, seq in enumerate(seq_names):\n",
    "            print(seq, modal)\n",
    "            !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} {'musthave='+modal} --remove_low_attributions {remove_edges_with_less} --dpi {dpi}\n",
    "            jpgs.append(f\"{attarrays.rsplit('.', 1)[0]}_{seq}_{modal}.jpg\")\n",
    "else:\n",
    "    for s, seq in enumerate(seq_names):\n",
    "        print(seq)\n",
    "        !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} 'all' --remove_low_attributions {remove_edges_with_less} --dpi {dpi}\n",
    "        jpgs.append(f\"{attarrays.rsplit('.', 1)[0]}_{seq}_all.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1961fd54-834c-47f1-aead-e7ecd1b542ac",
   "metadata": {
    "id": "1961fd54-834c-47f1-aead-e7ecd1b542ac",
    "outputId": "85bb4414-16b8-4ae2-8145-6c96585b4187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavg_Cd19_all.jpg\n"
     ]
    }
   ],
   "source": [
    "# The best would be to automate this and show all the generated files.\n",
    "from IPython.display import Image\n",
    "for jpg in jpgs:\n",
    "    print(jpg)\n",
    "    Image(filename=jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a66abd-63de-41da-9b5c-c11b5dc54389",
   "metadata": {},
   "source": [
    "## Save all selected attributions as bigwig files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1489b0b7-7a6a-4121-90a2-dc8b635326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromsizes_path = './data/mm10/chromsizes.pkl'\n",
    "pos_info_path = f'{sequencenpz.split(\"_\")[0]}_pos_info.pkl' # get this path based on sequencenpz path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "698b271e-09fd-413b-854c-ad5ff215a7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cd19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cd19 Cd19 ['B.ATAC' 'B.CTCF' 'B.H33' 'B.H3K27ac' 'B.H3K27me3' 'B.H3K36me3'\n",
      " 'B.H3K4me1' 'B.H3K4me3']\n",
      "(8, 2000, 4)\n"
     ]
    }
   ],
   "source": [
    "from drg_tools.io_utils import readinfasta\n",
    "\n",
    "if create_sequence:\n",
    "    seq_names, seqs = readinfasta(original_sequence)\n",
    "    sequencenpz = original_sequencenpz\n",
    "else:\n",
    "    seq_names, seqs = readinfasta(sequence)\n",
    "\n",
    "if plotpermodality:\n",
    "    for modal in modalities:\n",
    "        for s, seq in enumerate(seq_names):\n",
    "            print(seq, modal, attarrays, sequencenpz)\n",
    "            !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} {'musthave='+modal} --chromsizes_path {chromsizes_path} --pos_info_path {pos_info_path} --'save_bw'\n",
    "else:\n",
    "    for s, seq in enumerate(seq_names):\n",
    "        print(seq)\n",
    "        !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} 'all' --chromsizes_path {chromsizes_path} --pos_info_path {pos_info_path} --'save_bw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b99c44-6021-43d8-9b8d-3dfbbebe21f3",
   "metadata": {},
   "source": [
    "## Extract seqlets from attributions, cluster seqlets, normalize cluster motifs to use with tomtom \n",
    "#### These steps are from https://github.com/LXsasse/DRG/blob/main/examples/Attribution_analysis.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ee376-56b3-40a1-8023-87e84eca08b8",
   "metadata": {},
   "source": [
    "#### Extract seqlets from attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "04e768f8-5340-47c3-bf9f-b60578c0c69c",
   "metadata": {
    "id": "04e768f8-5340-47c3-bf9f-b60578c0c69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant cut off for p-value 5e-06 is z-score 4.6\n",
      "(15, 8, 4, 2000)\n",
      "20 extracted pwms from attributions of shape (1, 8, 2000)\n",
      "Saved pwms to ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavgall_globalmotifs4.6_1_4.npz\n",
      "Seqlet effects saved to ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavgall_globalmotifs4.6_1_4.npz\n"
     ]
    }
   ],
   "source": [
    "from drg_tools.stats_functions import pvalue2zscore\n",
    "\n",
    "pcut = 0.01/input_length # p-value cut off for significant motifs\n",
    "# convert p-value to z-score\n",
    "sigcut=round(pvalue2zscore(pcut, alternative='two-sided'),1) # z-score cut off for significant motifs\n",
    "print(f'Significant cut off for p-value {pcut} is z-score {sigcut}')\n",
    "\n",
    "# compute the absolute value of sigcut by multiplying with the standard deviation of the shuffled attributions\n",
    "attributions = np.load(outdir+'from'+model_file+'_'+attribution_type+'all.npz', allow_pickle=True)\n",
    "att_values = attributions['values']\n",
    "print(att_values.shape)\n",
    "\n",
    "maxgap=1 # maximum gap length\n",
    "minsize=4 # minimum number of significant bases in a 'motif'\n",
    "norm='global' # Z-score normalization derived from all attributions. Alternatiely, 'seq' if each sequence should be normalized individually, 'condition' if all sequences in a track should be normalized individually, 'std 0.1' if we want to devide by the standard deviation of 0.1 \n",
    "std =  np.std(att_values)\n",
    "seltracks = 'all' # select all tracks for motif extraction, or a specific track name e.g. 'B.Fem.Sp.ATAC'\n",
    "!python {drgclis+'sequence_attributions/run_extract_motifs_from_attributionmaps.py'} {attarrays} {sequencenpz} {sigcut} {maxgap} {minsize} {norm} {std} --select_tracks {seltracks}\n",
    "\n",
    "seqleteffects=f'{attarrays.split('.npz')[0]}{seltracks}_{norm}motifs{sigcut}_{maxgap}_{minsize}.txt'\n",
    "seqlets=f'{attarrays.split('.npz')[0]}{seltracks}_{norm}motifs{sigcut}_{maxgap}_{minsize}.npz'\n",
    "print(f'Seqlet effects saved to {seqlets}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7605a56-40b4-4af0-b628-647db39e6bc8",
   "metadata": {},
   "source": [
    "#### Cluster seqlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c3d114db-cb03-4155-93f5-f2c509de059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavgall_globalmotifs4.6_1_4ms4_cldcomplete0.05corpva.txt\n",
      "20 form 5 clusters\n",
      "5\n",
      "Saved PWM File as : ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavgall_globalmotifs4.6_1_4ms4_cldcomplete0.05corpvapfms.meme\n",
      "3 3\n",
      "4 1\n",
      "7 1\n"
     ]
    }
   ],
   "source": [
    "!python {drgclis+'interpret_models/compute_pwm_correlation_cluster_and_combine.py'} {seqlets} complete --distance_threshold 0.05 --distance_metric correlation_pvalue --clusternames\n",
    "\n",
    "seqlet_clusters=f'{os.path.splitext(seqlets)[0]}ms4_cldcomplete0.05corpva.txt'\n",
    "seqlet_clustermotifs=f'{os.path.splitext(seqlets)[0]}ms4_cldcomplete0.05corpvapfms.meme'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5e0a2-58bf-4fb2-9284-2b23968a3f57",
   "metadata": {},
   "source": [
    "#### Normalize cluster motifs to use with tomtom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5a31a10b-6a93-4f0b-98a2-76e302c4f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: use --custom_outname to avoid errors based on automatically generated filename being too long \n",
    "\n",
    "# TODO: Try different normalization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1a77a6c0-5f92-42ff-8b7f-6d525a36e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr:['strip', 0]\n",
      "5\n",
      "Saved PWM File as : ./results/norm_cluster_motifs.meme\n"
     ]
    }
   ],
   "source": [
    "strip=0 # To avoid 'No entries in pwm' error from parse_motifs_tomeme, set strip=0\n",
    "norm_cluster_outname = 'norm_cluster_motifs'\n",
    "!python {drgclis+'interpret_models/parse_motifs_tomeme.py'} {seqlet_clustermotifs} --transform exp,strip={strip},norm --'custom_outname' {norm_cluster_outname}\n",
    "norm_seqlet_clustermotifs = f'{os.path.dirname(seqlets)}/{norm_cluster_outname}.meme'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa1a5c-4e2d-46e2-8fa5-f7440f02774c",
   "metadata": {},
   "source": [
    "## Use tomtom to match cluster motif to a database, and save .bed files for the positions of significant motif matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fdecb6dd-461c-41f2-a19a-68ad973fa9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the number of .bed files saved will be number of sequences x number of tracks \n",
    "\n",
    "# TODO: Try different motif database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_data_path='./data/ImmGen_CutRun_RNAseq_all_1_adjust.gct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf478d4-4733-4339-a145-0df647438b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tomtom matches\n",
      "searching 5 queries against 791 targets\n",
      "saving .bed files\n",
      "Saving 1 motifs to ./results/Cd19_B.H3K27me3.bed\n",
      "Saving 1 motifs to ./results/Cd19_B.CTCF.bed\n",
      "Saving 3 motifs to ./results/Cd19_B.H3K27ac.bed\n",
      "Saving 2 motifs to ./results/Cd19_B.H3K36me3.bed\n",
      "Saving 13 motifs to ./results/Cd19_B.ATAC.bed\n"
     ]
    }
   ],
   "source": [
    "motif_database_path='./data/mouse_pfms_v4.meme'\n",
    "motif_database_path = os.path.abspath('./data/JASPAR2020_CORE_vertebrates_non-redundant_pfms.TFnames.meme')  # Ensure absolute path for the motif database\n",
    "motif_database_path = os.path.abspath('./data/JASPAR2022_CORE_vertebrates_non-redundant_v2TF.meme')\n",
    "save_dir = './results/' # where the .bed files are saved\n",
    "!python  {drgclis+'sequence_attributions/save_motif_labels_as_bed.py'} --motif_database_path {motif_database_path} --seqlet_cwm_path {norm_seqlet_clustermotifs} --seqlet_info_path {seqlet_clusters} --pos_info_path {pos_info_path} --save_dir {save_dir} --expr_data_path {expr_data_path} --lineage_data_path {lineage_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609956ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
