{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c764bb5-d5d9-4829-ac0e-20aebb4934e6",
   "metadata": {
    "id": "8c764bb5-d5d9-4829-ac0e-20aebb4934e6"
   },
   "source": [
    "# The webserver notebook\n",
    "Make sure you have both model files in `/data/models/`. The model can be initialized with `*_model_params.dat`. The python script will then automatically read in the matching `*_parameter.pth` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e05bae2-6d44-4d87-b4ef-f382fa5330b9",
   "metadata": {
    "id": "5e05bae2-6d44-4d87-b4ef-f382fa5330b9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Specify your model\n",
    "model_file='CTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F'\n",
    "model_params = './models/'+model_file+'_model_params.dat'\n",
    "\n",
    "# Specify path to drg_tools\n",
    "drgclis='~/Git/DRG/scripts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86828f-f0a0-4a13-858b-fc85f0cec2df",
   "metadata": {
    "id": "6c86828f-f0a0-4a13-858b-fc85f0cec2df"
   },
   "source": [
    "## Specify input to server\n",
    "The model takes 2000bp sequence intervals as input and predicts the ATAC-seq signal within a 250bp window in the center of that interval, as well as the signal in 1000 bp windows of 7 Cut&Run data sets. The sequence attributions show all bases and motifs that contribute to the center window only. \n",
    "\n",
    "1. Determine if you want to provide genomic location of a fasta file.\n",
    "\n",
    "    a. if provided a sequence, it will either be cut into overlapping 2000 bp fragments shifted by 250bp, or padded to 2000bp.\n",
    "\n",
    "2. If given a location \n",
    "    \n",
    "    a. determine if you want the attribution for the signal in this region (i.e. all attributions that contribute to a window in this region, **output centric view**) or for the sequence (i.e. all attributions that that fall into this interval, **input centric view**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f549ec6d-d0b8-4d77-bbf0-4d18cce3c573",
   "metadata": {
    "id": "f549ec6d-d0b8-4d77-bbf0-4d18cce3c573"
   },
   "outputs": [],
   "source": [
    "# Specify model input\n",
    "create_sequence = True\n",
    "bed = './data/inputs/test.bed'\n",
    "original_bed = './data/inputs/test.bed'\n",
    "input_centric = False\n",
    "sequence = './data/inputs/mm10test2000.fasta' # only used if bed given and create_sequence set to False.\n",
    "input_length = 2000 # length of the input sequence\n",
    "atac_window_size = 250\n",
    "cr_window_size = 1000\n",
    "window_size = min(atac_window_size, cr_window_size)\n",
    "step_size = window_size // 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c814ecd-5ef2-4fd3-818e-987ee3300e4c",
   "metadata": {
    "id": "4c814ecd-5ef2-4fd3-818e-987ee3300e4c",
    "outputId": "4682a73d-d0af-47e7-ce77-bca432d2396d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 7 sequences of length 2000 for 1000 region with input_centric=False.\n",
      "Data format (1, 6)\n",
      "['chr1']\n",
      "./data/inputs/mm10test\n",
      "Read ./data/mm10/chr1.fa.gz\n",
      "Length 195471971\n",
      "Generate seq for chr1\n",
      "Locations in chr 1\n",
      "Max sequence length ImmGenATAC1219.peak_3 1000\n",
      "Saved as \n",
      "./data/inputs/mm10test_onehot-ACGT_alignleft.npz\n",
      "Data format (7, 4)\n",
      "['chr1']\n",
      "./data/inputs/mm10test_oc2000\n",
      "Read ./data/mm10/chr1.fa.gz\n",
      "Length 195471971\n",
      "Generate seq for chr1\n",
      "Locations in chr 7\n",
      "Max sequence length ImmGenATAC1219.peak_3_cm0 2000\n",
      "Saved as \n",
      "./data/inputs/mm10test_oc2000_onehot-ACGT_alignleft.npz\n"
     ]
    }
   ],
   "source": [
    "# Create necessary input files for the model\n",
    "# Note: include --'save_pos_info' so that this info can be used to create .bw and .bed files for Webserver\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "bed = './data/inputs/test.bed'\n",
    "mm10='./data/mm10' # contains all chr1-19 in as chr1.fa.gz\n",
    "\n",
    "def create_centric_bed_file(bed, window_size, step_size, input_length, input_centric):\n",
    "    \"\"\"\n",
    "    Create a bed file with 2000bp sequences for all 250 bp windows in the center of the 2000bp sequence.\n",
    "    If input_centric is True, All 2000bp sequences that overlap with at least step_size with the region in the bed file are created.\n",
    "    If input_centric is False, only the 2000pb sequences whose center 250bp overlap with the region in the bed file are created.\n",
    "    The new bed file is saved with '_ic.bed' or '_oc.bed' suffix depending on the value of input_centric.\n",
    "    250bp windows are created in the center of the 2000bp sequence.\n",
    "    \"\"\"\n",
    "    newbed = bed.replace('.bed', '_ic.bed' if input_centric else '_oc.bed')\n",
    "    newfile = open(newbed, 'w')\n",
    "    for line in open(bed):\n",
    "        if not line.startswith('#'):\n",
    "            fields = line.strip().split()\n",
    "            chrom, start, end, name = fields[0], int(fields[1]), int(fields[2]), fields[3]\n",
    "            center = (start + end) // 2\n",
    "            region_length = end - start\n",
    "            newfile.write(f\"{chrom}\\t{center-step_size}\\t{center + step_size}\\t{name}_p\\n\")\n",
    "            # Need to add the input_length/2 only if input_centric is True\n",
    "            n_splits = int((region_length + input_length * int(input_centric)) / (2 * step_size)) - 1 + int((region_length + input_length * int(input_centric)) % (2 * step_size) > 0)\n",
    "            print(f'Creating {2*n_splits+1} sequences of length {input_length} for {region_length} region with input_centric={input_centric}.')\n",
    "            for i in range(int((region_length+input_length*int(input_centric))/2/step_size)-1+int((region_length+input_length*int(input_centric))%(2*step_size)>0)):\n",
    "                newfile.write(f\"{chrom}\\t{center + i*step_size}\\t{center + i*step_size + window_size}\\t{name}_cp{i}\\n\")\n",
    "                newfile.write(f\"{chrom}\\t{center -i*step_size-window_size}\\t{center - i*step_size}\\t{name}_cm{i}\\n\")\n",
    "    newfile.close()\n",
    "    return newbed\n",
    "\n",
    "\n",
    "if create_sequence:\n",
    "\n",
    "    newbed = create_centric_bed_file(bed, window_size, step_size, input_length, input_centric)\n",
    "    # If input_centric is True, the new bed file will have '_ic.bed' suffix, otherwise '_oc.bed'\n",
    "\n",
    "    original_bed = bed\n",
    "    bed = newbed\n",
    "\n",
    "    prefix = os.path.split(bed)[0]\n",
    "    if prefix != '':\n",
    "        prefix = prefix+'/'\n",
    "    sequence = prefix+mm10.strip('/').split('/')[-1]+os.path.splitext(os.path.split(bed.strip('.gz'))[1])[0]+f'{input_length}.fasta'\n",
    "    original_sequence = prefix+mm10.strip('/').split('/')[-1]+os.path.splitext(os.path.split(original_bed.strip('.gz'))[1])[0]+'.fasta'\n",
    "    \n",
    "    # Create fasta form mm10 genome with bed file\n",
    "    !python {drgclis+'data_preprocessing/generate_fasta_from_bedgtf_and_genome.py'} {mm10} {original_bed} --'save_pos_info'\n",
    "    # Create one-hot encoding for input to model and attributions\n",
    "    !python {drgclis+'data_preprocessing/transform_seqtofeature.py'} {original_sequence}\n",
    "    original_sequencenpz=f'{os.path.splitext(original_sequence)[0]}_onehot-ACGT_alignleft.npz'\n",
    "\n",
    "\n",
    "# Create fasta form mm10 genome with bed file\n",
    "!python {drgclis+'data_preprocessing/generate_fasta_from_bedgtf_and_genome.py'} {mm10} {bed} --extend_to_length {input_length} --'save_pos_info'\n",
    "\n",
    "# Potentially create background shuffled sequences for motif detection\n",
    "# Attribution from shuffled sequences will be used to determine significant acttributions in the actual sequences.\n",
    "# If you want to use the background sequences, set the variable 'background' to True\n",
    "background = False\n",
    "if background:\n",
    "    # read fasta file and create background sequences\n",
    "    from drg_tools.io_utils import readinfasta\n",
    "    seq_names, seqs = readinfasta(sequence)\n",
    "    from tangermeme.ersatz import dinucleotide_shuffle\n",
    "    from tangermeme.utils import one_hot_encode\n",
    "    import torch\n",
    "    ohseq = np.array([one_hot_encode(seq) for seq in seqs])\n",
    "    print('Original sequences:', ohseq.shape)\n",
    "    shuffled_seqs = dinucleotide_shuffle(torch.tensor(ohseq), n=1).squeeze(1).numpy()\n",
    "    print('Shuffled sequences:', shuffled_seqs.shape)\n",
    "    rseq_names = ['shuffled_'+name for name in seq_names]\n",
    "    print(rseq_names)\n",
    "    # convert shuffled sequences back to string format\n",
    "    shuffled_seqs = [''.join(np.array(list('ACGT'))[np.argmax(base, axis = -2)]) for base in shuffled_seqs]\n",
    "    # add the shuffled sequences to the sequence file\n",
    "    with open(sequence, 'a') as f:\n",
    "        for name, seq in zip(rseq_names, shuffled_seqs):\n",
    "            print(name, seq)\n",
    "            f.write(f'>{name}\\n'+''.join(seq) + '\\n')\n",
    "\n",
    "\n",
    "# Create one-hot encoding for input to model and attributions\n",
    "!python {drgclis+'data_preprocessing/transform_seqtofeature.py'} {sequence}\n",
    "sequencenpz=f'{os.path.splitext(sequence)[0]}_onehot-ACGT_alignleft.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed995a01-8108-423b-8510-037e4fc74de5",
   "metadata": {
    "id": "ed995a01-8108-423b-8510-037e4fc74de5"
   },
   "source": [
    "## Specificy output from server\n",
    "Now, let's specify what we want to get from the model. Below are three different possible choices\n",
    "1. Return attributions for lineages and all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48532ad-de96-4150-8a66-dccbd96d2a7e",
   "metadata": {
    "id": "a48532ad-de96-4150-8a66-dccbd96d2a7e"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'all' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'all' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = True # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = True # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90534f-0ff5-488b-b1a8-587066867510",
   "metadata": {
    "id": "9f90534f-0ff5-488b-b1a8-587066867510"
   },
   "source": [
    "2. Return attributions for only one cell type and one modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9d6a576-802f-4951-bb6f-b388a733ee97",
   "metadata": {
    "id": "c9d6a576-802f-4951-bb6f-b388a733ee97"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'B.Fem.Sp' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'ATAC' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = False # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = True # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190b255-918c-4e21-9365-d7f967c60d7a",
   "metadata": {
    "id": "0190b255-918c-4e21-9365-d7f967c60d7a"
   },
   "source": [
    "3. Compare B-cell attributions across modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584343aa-71fc-43e8-a76d-838d5aa1b013",
   "metadata": {
    "id": "584343aa-71fc-43e8-a76d-838d5aa1b013"
   },
   "outputs": [],
   "source": [
    "return_attribution = True # If False, only returns predictions\n",
    "attribution_type = 'grad' # only grad or deepshap are feasible for large sequences. deepshap is not recommended for model with weighted avg pooling\n",
    "cell_types = 'B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp,B.GC.CC.Sp,B.MZ.Sp,B.PB.Sp,B.PC.BM,B.PC.Sp,B.Sp,B.T1.Sp,B.T2.Sp,B.T3.Sp,B.mem.Sp,B1b.PC' # or specific output tracks separated by ',', e.g. B.Fem.Sp,B.Fo.Sp,B.FrE.BM,B.GC.CB.Sp\n",
    "modalities = 'all' # Define data modalities that you want to look at: choose from ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3\n",
    "lineages = True # Define if Attributions should be summarized to lineages\n",
    "plotpermodality = False # Determine if a separate figure is generated for each modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "413146d2-eded-4d61-94f4-5b0992856c2e",
   "metadata": {
    "id": "413146d2-eded-4d61-94f4-5b0992856c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ATAC'] ['B.Fem.Sp']\n",
      "Selected tracks: --select_tracks B.Fem.Sp.ATAC\n"
     ]
    }
   ],
   "source": [
    "# Use the given input arguments to generate inputs for the model\n",
    "import pandas as pd\n",
    "\n",
    "# Load master file for lineages for selection and potential lineage summary\n",
    "lineage_file = './data/CutandRun_and_ATAC.lineages.txt'\n",
    "lineage_frame = pd.read_table(lineage_file, header = None, names = ['cell_type', 'lineage'])\n",
    "data_modalities = 'ATAC,CTCF,H33,H3K4me1,H3K4me3,H3K27me3,H3K27ac,H3K36me3'.split(',')\n",
    "outdir='./results/' # directory to save files to\n",
    "\n",
    "# Select tracks that will be returned\n",
    "if cell_types == 'all':\n",
    "    cell_types = lineage_frame['cell_type']\n",
    "elif ',' in cell_types:\n",
    "    cell_types = cell_types.split(',')\n",
    "else: # for single cell type\n",
    "    cell_types = [cell_types]\n",
    "\n",
    "if modalities == 'all':\n",
    "    modalities = data_modalities\n",
    "elif ',' in modalities:\n",
    "    modalities = modalities.split(',')\n",
    "else: # for single modality\n",
    "    modalities = [modalities]\n",
    "print(modalities, cell_types)\n",
    "tracks = '--select_tracks '\n",
    "for modal in modalities:\n",
    "    for cell_type in cell_types:\n",
    "        tracks += f'{cell_type}.{modal},'\n",
    "tracks = tracks.strip(',')\n",
    "print(f\"Selected tracks: {tracks}\")\n",
    "\n",
    "# Average over cell lineages\n",
    "mean_lineage = ''\n",
    "mean_lineage_file = './data/CutandRun_and_ATAC.lineages.modsep.tsv'\n",
    "if lineages:\n",
    "    mean_lineage = f'--average_outclasses {mean_lineage_file}'\n",
    "\n",
    "# Define sequence attribuitons that should be returned\n",
    "seq_atts = ''\n",
    "if return_attribution:\n",
    "    seq_atts = f'--sequence_attributions {attribution_type} all'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89bcb6-279a-43a1-b398-e17c44157f0d",
   "metadata": {
    "id": "ea89bcb6-279a-43a1-b398-e17c44157f0d"
   },
   "source": [
    "## Run model to compute predictions (and attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "193994e3-086e-4342-b831-91adc56758ff",
   "metadata": {
    "id": "193994e3-086e-4342-b831-91adc56758ff",
    "outputId": "ff83473f-5550-4062-875e-aac827bb3da9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ~/Git/DRG/scripts/train_models/run_cnn_model.py ./data/inputs/mm10test_oc2000_onehot-ACGT_alignleft.npz None --predictnew --cnn ./models/CTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_model_params.dat device=cpu --load_output_track_names ./data/CTCF_tracks.txt,./data/H3K27ac_tracks.txt,./data/H3K36me3_tracks.txt,./data/H3K4me3_tracks.txt,./data/H33_tracks.txt,./data/H3K27me3_tracks.txt,./data/H3K4me1_tracks.txt,./data/ATAC_tracks.txt --select_tracks B.Fem.Sp.ATAC --save_predictions --sequence_attributions grad all  --outname ./results/\n",
      "None is not a valid file.\n",
      "Input shapes X: (7, 4, 2000)\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Selected list of tracks do not match the names in the data\n",
      "Device cpu\n",
      "In features 4 2000\n",
      "Convolutions 512 2000\n",
      "Pooling 512 200\n",
      "Convolution after attention 512 8\n",
      "Before FCL 4096\n",
      "outclasses [67, 93, 77, 81, 56, 94, 80, 81]\n",
      "Loaded convolutions.conv1d.weight with convolutions.conv1d.weight\n",
      "Loaded convolutions.conv1d.bias with convolutions.conv1d.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.weight with trconvolution_layers.convlayers.Bnorm0.weight\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.bias with trconvolution_layers.convlayers.Bnorm0.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.running_mean with trconvolution_layers.convlayers.Bnorm0.running_mean\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.running_var with trconvolution_layers.convlayers.Bnorm0.running_var\n",
      "Loaded trconvolution_layers.convlayers.Bnorm0.num_batches_tracked with trconvolution_layers.convlayers.Bnorm0.num_batches_tracked\n",
      "Loaded trconvolution_layers.convlayers.Conv0.conv1d.weight with trconvolution_layers.convlayers.Conv0.conv1d.weight\n",
      "Loaded trconvolution_layers.convlayers.Conv0.conv1d.bias with trconvolution_layers.convlayers.Conv0.conv1d.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.weight with trconvolution_layers.convlayers.Bnorm1.weight\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.bias with trconvolution_layers.convlayers.Bnorm1.bias\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.running_mean with trconvolution_layers.convlayers.Bnorm1.running_mean\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.running_var with trconvolution_layers.convlayers.Bnorm1.running_var\n",
      "Loaded trconvolution_layers.convlayers.Bnorm1.num_batches_tracked with trconvolution_layers.convlayers.Bnorm1.num_batches_tracked\n",
      "Loaded trconvolution_layers.convlayers.Conv1.conv1d.weight with trconvolution_layers.convlayers.Conv1.conv1d.weight\n",
      "Loaded trconvolution_layers.convlayers.Conv1.conv1d.bias with trconvolution_layers.convlayers.Conv1.conv1d.bias\n",
      "Loaded nfcs.0.nfcs.EmbeddtoFully.weight with nfcs.0.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.0.nfcs.EmbeddtoFully.bias with nfcs.0.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected0.weight with nfcs.0.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected0.bias with nfcs.0.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected1.weight with nfcs.0.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected1.bias with nfcs.0.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.0.nfcs.Fullyconnected2.weight with nfcs.0.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.0.nfcs.Fullyconnected2.bias with nfcs.0.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.1.nfcs.EmbeddtoFully.weight with nfcs.1.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.1.nfcs.EmbeddtoFully.bias with nfcs.1.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected0.weight with nfcs.1.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected0.bias with nfcs.1.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected1.weight with nfcs.1.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected1.bias with nfcs.1.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.1.nfcs.Fullyconnected2.weight with nfcs.1.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.1.nfcs.Fullyconnected2.bias with nfcs.1.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.2.nfcs.EmbeddtoFully.weight with nfcs.2.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.2.nfcs.EmbeddtoFully.bias with nfcs.2.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected0.weight with nfcs.2.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected0.bias with nfcs.2.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected1.weight with nfcs.2.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected1.bias with nfcs.2.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.2.nfcs.Fullyconnected2.weight with nfcs.2.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.2.nfcs.Fullyconnected2.bias with nfcs.2.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.3.nfcs.EmbeddtoFully.weight with nfcs.3.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.3.nfcs.EmbeddtoFully.bias with nfcs.3.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected0.weight with nfcs.3.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected0.bias with nfcs.3.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected1.weight with nfcs.3.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected1.bias with nfcs.3.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.3.nfcs.Fullyconnected2.weight with nfcs.3.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.3.nfcs.Fullyconnected2.bias with nfcs.3.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.4.nfcs.EmbeddtoFully.weight with nfcs.4.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.4.nfcs.EmbeddtoFully.bias with nfcs.4.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected0.weight with nfcs.4.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected0.bias with nfcs.4.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected1.weight with nfcs.4.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected1.bias with nfcs.4.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.4.nfcs.Fullyconnected2.weight with nfcs.4.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.4.nfcs.Fullyconnected2.bias with nfcs.4.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.5.nfcs.EmbeddtoFully.weight with nfcs.5.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.5.nfcs.EmbeddtoFully.bias with nfcs.5.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected0.weight with nfcs.5.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected0.bias with nfcs.5.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected1.weight with nfcs.5.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected1.bias with nfcs.5.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.5.nfcs.Fullyconnected2.weight with nfcs.5.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.5.nfcs.Fullyconnected2.bias with nfcs.5.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.6.nfcs.EmbeddtoFully.weight with nfcs.6.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.6.nfcs.EmbeddtoFully.bias with nfcs.6.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected0.weight with nfcs.6.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected0.bias with nfcs.6.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected1.weight with nfcs.6.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected1.bias with nfcs.6.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.6.nfcs.Fullyconnected2.weight with nfcs.6.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.6.nfcs.Fullyconnected2.bias with nfcs.6.nfcs.Fullyconnected2.bias\n",
      "Loaded nfcs.7.nfcs.EmbeddtoFully.weight with nfcs.7.nfcs.EmbeddtoFully.weight\n",
      "Loaded nfcs.7.nfcs.EmbeddtoFully.bias with nfcs.7.nfcs.EmbeddtoFully.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected0.weight with nfcs.7.nfcs.Fullyconnected0.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected0.bias with nfcs.7.nfcs.Fullyconnected0.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected1.weight with nfcs.7.nfcs.Fullyconnected1.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected1.bias with nfcs.7.nfcs.Fullyconnected1.bias\n",
      "Loaded nfcs.7.nfcs.Fullyconnected2.weight with nfcs.7.nfcs.Fullyconnected2.weight\n",
      "Loaded nfcs.7.nfcs.Fullyconnected2.bias with nfcs.7.nfcs.Fullyconnected2.bias\n",
      "Loaded classifier.0.classifier.Linear.weight with classifier.0.classifier.Linear.weight\n",
      "Loaded classifier.0.classifier.Linear.bias with classifier.0.classifier.Linear.bias\n",
      "Loaded classifier.1.classifier.Linear.weight with classifier.1.classifier.Linear.weight\n",
      "Loaded classifier.1.classifier.Linear.bias with classifier.1.classifier.Linear.bias\n",
      "Loaded classifier.2.classifier.Linear.weight with classifier.2.classifier.Linear.weight\n",
      "Loaded classifier.2.classifier.Linear.bias with classifier.2.classifier.Linear.bias\n",
      "Loaded classifier.3.classifier.Linear.weight with classifier.3.classifier.Linear.weight\n",
      "Loaded classifier.3.classifier.Linear.bias with classifier.3.classifier.Linear.bias\n",
      "Loaded classifier.4.classifier.Linear.weight with classifier.4.classifier.Linear.weight\n",
      "Loaded classifier.4.classifier.Linear.bias with classifier.4.classifier.Linear.bias\n",
      "Loaded classifier.5.classifier.Linear.weight with classifier.5.classifier.Linear.weight\n",
      "Loaded classifier.5.classifier.Linear.bias with classifier.5.classifier.Linear.bias\n",
      "Loaded classifier.6.classifier.Linear.weight with classifier.6.classifier.Linear.weight\n",
      "Loaded classifier.6.classifier.Linear.bias with classifier.6.classifier.Linear.bias\n",
      "Loaded classifier.7.classifier.Linear.weight with classifier.7.classifier.Linear.weight\n",
      "Loaded classifier.7.classifier.Linear.bias with classifier.7.classifier.Linear.bias\n",
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F\n",
      "SAVED ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_pred.npz (7, 1)\n",
      "TISM time for (7, 1, 4, 2000) 1 0.49460935592651367\n",
      "Attributions saved as ./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradall.npz\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "## determine if you have a gpu for computations\n",
    "device='cpu'\n",
    "\n",
    "# keep track name files in that order because it's the same as during training.\n",
    "track_names='--load_output_track_names ./data/CTCF_tracks.txt,./data/H3K27ac_tracks.txt,./data/H3K36me3_tracks.txt,./data/H3K4me3_tracks.txt,./data/H33_tracks.txt,./data/H3K27me3_tracks.txt,./data/H3K4me1_tracks.txt,./data/ATAC_tracks.txt'\n",
    "print(f\"python {drgclis+'train_models/run_cnn_model.py'} {sequencenpz} None --predictnew --cnn {model_params} {'device='+device} {track_names} {tracks} --save_predictions {seq_atts} {mean_lineage} --outname {outdir}\")\n",
    "!python {drgclis+'train_models/run_cnn_model.py'} {sequencenpz} None --predictnew --cnn {model_params} {'device='+device} {track_names} {tracks} --save_predictions {seq_atts} {mean_lineage} --outname {outdir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651baa7",
   "metadata": {},
   "source": [
    "### In case of an output centric view sum all the individual attributions over the original window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d15fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution names: ['ImmGenATAC1219.peak_3_cm0' 'ImmGenATAC1219.peak_3_cm1'\n",
      " 'ImmGenATAC1219.peak_3_cm2' 'ImmGenATAC1219.peak_3_cp0'\n",
      " 'ImmGenATAC1219.peak_3_cp1' 'ImmGenATAC1219.peak_3_cp2'\n",
      " 'ImmGenATAC1219.peak_3_p']\n",
      "Attribution average shape: (1, 1, 4, 1000)\n",
      "Attribution average shape: (1, 1, 4, 1000), names: ['ImmGenATAC1219.peak_3']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_attributions_over_windows(original_bed_path, att_names, att_values, att_exp, input_length, input_centric, \n",
    "                                      step_size, combination_method='average', combination_window_size=250):\n",
    "    \"\"\"\n",
    "    Average attributions over all windows in the original sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_bed_path : str\n",
    "        Path to the original bed file with regions to average over\n",
    "    att_names : np.array\n",
    "        Array of individual attribution names, need to be in the format 'name_p', 'name_cp0', 'name_cm0', etc.\n",
    "    att_values : np.array\n",
    "        Array of individual attribution values\n",
    "    att_exp : np.array\n",
    "        Array of experiments, only required when combination_method is 'weighted'\n",
    "    input_length : int\n",
    "        Length of the attributions \n",
    "    input_centric : bool\n",
    "        Whether input centric view is used\n",
    "    step_size : int\n",
    "        Step size for sliding windows\n",
    "    combination_method : str, optional\n",
    "        Method to combine attributions, can be 'average', 'weighted', or 'max'. Default is 'average', and only 'average' is implemented.\n",
    "    combination_window_size : int, or dict, optional\n",
    "        Size of the window for combination, only used if combination_method is 'weighted'. Default is 250 for all modalities.\n",
    "        Can be a single integer or a dictionary with modality names as keys and window sizes as values.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (attribution_average, attribution_names, att_exp)\n",
    "        - attribution_average: averaged attributions array\n",
    "        - attribution_names: names array for averaged attributions\n",
    "        - att_exp: experiments array (unchanged)\n",
    "    \"\"\"\n",
    "    # Read in original bed file\n",
    "    bed_obj = open(original_bed_path, 'r')\n",
    "    \n",
    "    attribution_average = []\n",
    "    attribution_names = []\n",
    "    \n",
    "    # Average over all windows in the original sequence\n",
    "    for region in bed_obj:\n",
    "        if not region.startswith('#'):\n",
    "            fields = region.strip().split()\n",
    "            chrom, start, end, name = fields[0], int(fields[1]), int(fields[2]), fields[3]\n",
    "            region_length = end - start\n",
    "\n",
    "            # Get the attributions for the current region\n",
    "            att_average = att_values[att_names == name + '_p']\n",
    "\n",
    "            n_splits = int((end-start+input_length*int(input_centric))/2/step_size)-1+int((end-start+input_length*int(input_centric))%(2*step_size)>0)\n",
    "            n_ = n_splits * 2 + 1\n",
    "            att_average = (1/n_) *att_average[..., max(0, (input_length-region_length)//2): min(input_length, region_length + (input_length-region_length)//2)]\n",
    "            print(f\"Attribution average shape: {att_average.shape}\")\n",
    "            \n",
    "            for i in range(n_splits):\n",
    "                att_namep = f'{name}_cp{i}'\n",
    "                att_namem = f'{name}_cm{i}'\n",
    "                if att_namep in att_names:\n",
    "                    idx = np.where(att_names == att_namep)[0]\n",
    "                    # Add the attributions to the original sequence\n",
    "                    # Calculate where the start would be given region_length and input_length\n",
    "                    start_at = region_length//2 - (input_length//2 - step_size*(i+1))\n",
    "                    end_at = input_length//2 - (region_length//2) - step_size*(i+1)\n",
    "                    # If region_length//2 + step_size < input_length//2, we start 0 for assignment\n",
    "                    # Else if region_length//2 + step_size >= input_length//2, we start at region_length//2 + step_size - input_length//2\n",
    "                    start_average = max(0, start_at) # if start_at < 0, we start at 0\n",
    "                    # End average is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_average = min(input_length+start_average, region_length)\n",
    "                    # if input_length//2 > region_length//2+step_size*(i+1), not the entire region is mapped\n",
    "                    start_attr = max(0, end_at)\n",
    "                    # End attribute is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_attr = min(input_length, min(start_attr,end_at) + region_length)\n",
    "                    #print(np.corrcoef(att_average[..., start_average: end_average].flatten(), att_values[idx][..., start_attr: end_attr].flatten()))\n",
    "                    #print(np.corrcoef(att_average[..., start_average+1: end_average].flatten(), att_values[idx][..., start_attr: end_attr-1].flatten()))\n",
    "                    att_average[..., start_average: end_average] += (1/n_) * att_values[idx][..., start_attr: end_attr]\n",
    "                    #print(start_average, end_average, start_attr, end_attr, att_values[idx].shape)\n",
    "                if att_namem in att_names:\n",
    "                    idx = np.where(att_names == att_namem)[0]\n",
    "                    # Now do the indexing to the left side of the region\n",
    "                    # Calculate where the start would be given region_length and input_length\n",
    "                    start_at = region_length//2 - (input_length//2 + step_size*(i+1))\n",
    "                    end_at = input_length//2 - (region_length//2) - step_size*(i+1)\n",
    "                    # If region_length//2 + step_size < input_length//2, we start 0 for assignment\n",
    "                    # Else if region_length//2 + step_size >= input_length//2, we start at region_length//2 + step_size - input_length//2\n",
    "                    start_average = max(0, start_at)\n",
    "                    # End average is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_average = min(input_length+start_average, region_length + min(0, input_length//2-region_length//2-step_size*(i+1)))\n",
    "                    # if input_length//2 > region_length//2+step_size*(i+1), not the entire region is mapped\n",
    "                    start_attr = max(0, input_length//2 - (region_length//2) + step_size*(i+1))\n",
    "                    # End attribute is from the start to the end of the region_length or the end of the input_length\n",
    "                    end_attr = min(input_length, start_attr + region_length)\n",
    "                    # Add the attributions to the original sequence\n",
    "                    att_average[..., start_average: end_average] += (1/n_) * att_values[idx][..., start_attr: end_attr]\n",
    "                    #print(np.corrcoef(att_average[..., start_average: end_average].flatten(), att_values[idx][..., start_attr: end_attr].flatten()))\n",
    "                    #print(start_average, end_average, start_attr, end_attr)\n",
    "\n",
    "            attribution_average.append(att_average)\n",
    "            attribution_names.append(name)\n",
    "    \n",
    "    bed_obj.close()\n",
    "    \n",
    "    # Concatenate and convert to arrays\n",
    "    attribution_average = np.concatenate(attribution_average, axis = 0)\n",
    "    attribution_names = np.array(attribution_names)\n",
    "    print(f\"Attribution average shape: {attribution_average.shape}, names: {attribution_names}\")\n",
    "    \n",
    "    return attribution_average, attribution_names, att_exp\n",
    "\n",
    "\n",
    "# Combination methods: Average, Weighted, Max \n",
    "# Weighted would weight each window from the center differently with 1/((j+1)*n_), giving higher weights to intervals that are close to the window\n",
    "# We would multiply this weight tensor beforehand with the attribution values.\n",
    "# Need to generate two weighted tensors, one with atac_window_size and one with cr_window_size\n",
    "combination_method = 'average'\n",
    "\n",
    "# Create output centric view\n",
    "# Note: this is only possible if the sequence was created from a bed file.\n",
    "# Read in attributions\n",
    "attarrays=outdir+'from'+model_file+'_'+attribution_type+'all.npz'\n",
    "attributions = np.load(attarrays, allow_pickle=True)\n",
    "att_names = attributions['names']\n",
    "print(f\"Attribution names: {att_names}\")\n",
    "att_values = attributions['values']\n",
    "att_exp = attributions['experiments']\n",
    "\n",
    "# Call the function to average attributions\n",
    "attribution_average, attribution_names, att_exp = average_attributions_over_windows(\n",
    "    original_bed, att_names, att_values, att_exp, input_length, input_centric, step_size, combination_method\n",
    ")\n",
    "\n",
    "# Save the average attributions\n",
    "attarrays = attarrays.replace('.npz', 'avg.npz')\n",
    "np.savez_compressed(attarrays, values=attribution_average, names=attribution_names, experiments=att_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140900ce-228a-4967-8812-72fbe92f395b",
   "metadata": {
    "id": "140900ce-228a-4967-8812-72fbe92f395b"
   },
   "source": [
    "## Visualize all selected attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "714252ce-288d-4807-8277-2cb6b520eb85",
   "metadata": {
    "id": "714252ce-288d-4807-8277-2cb6b520eb85",
    "outputId": "a1a6b60b-65d7-4075-9758-56d434e305e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImmGenATAC1219.peak_3 ATAC\n",
      "ImmGenATAC1219.peak_3 ImmGenATAC1219.peak_3 ['B.Fem.Sp.ATAC']\n",
      "(1, 1000, 4)\n",
      "New range [np.int64(89), np.int64(890)]\n",
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradallavg_ImmGenATAC1219.peak_3_ATAC.jpg\n"
     ]
    }
   ],
   "source": [
    "# Visualize attributions\n",
    "# Some visualization choices\n",
    "remove_edges_with_less = 0.5 # removes left and right parts of the attribution map that don't have attribtuions larger than this fraction of the maximum in the sequence\n",
    "dpi=100 # resolution\n",
    "\n",
    "from drg_tools.io_utils import readinfasta\n",
    "#attarrays=outdir+'from'+model_file+'_'+attribution_type+'all.npz'\n",
    "if create_sequence:\n",
    "    seq_names, seqs = readinfasta(original_sequence)\n",
    "    sequencenpz = original_sequencenpz\n",
    "else:\n",
    "    seq_names, seqs = readinfasta(sequence)\n",
    "\n",
    "jpgs = []\n",
    "if plotpermodality:\n",
    "    for modal in modalities:\n",
    "        for s, seq in enumerate(seq_names):\n",
    "            print(seq, modal)\n",
    "            !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} {'musthave='+modal} --remove_low_attributions {remove_edges_with_less} --dpi {dpi}\n",
    "            jpgs.append(f'{attarrays.rsplit('.',1)[0]}_{seq}_{modal}.jpg')\n",
    "else:\n",
    "    for s, seq in enumerate(seq_names):\n",
    "        print(seq)\n",
    "        !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} 'all' --remove_low_attributions {remove_edges_with_less} --dpi {dpi}\n",
    "        jpgs.append(f'{attarrays.rsplit('.',1)[0]}_{seq}_all.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961fd54-834c-47f1-aead-e7ecd1b542ac",
   "metadata": {
    "id": "1961fd54-834c-47f1-aead-e7ecd1b542ac",
    "outputId": "85bb4414-16b8-4ae2-8145-6c96585b4187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/fromCTCFaH3K27acaH3K36me3aH3K4me3aH33aH3K27me3aH3K4me1aATAConseq2krcomp_mh0-cv10-1_Cormsek512l19TfEXPGELUmax10rcTvlCota_tc2dNoned1s1r1l7ma5nfc3s1024cbnoTfdo0.1tr1e-05SGD0.9bs64-F_gradall_ImmGenATAC1219.peak_3_all.jpg\n"
     ]
    }
   ],
   "source": [
    "# The best would be to automate this and show all the generated files.\n",
    "from IPython.display import Image\n",
    "for jpg in jpgs:\n",
    "    print(jpg)\n",
    "    Image(filename=jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230cdb6-1707-4cc3-8ab4-7025b70348d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a66abd-63de-41da-9b5c-c11b5dc54389",
   "metadata": {},
   "source": [
    "## Save all selected attributions as bigwig files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489b0b7-7a6a-4121-90a2-dc8b635326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromsizes_path = './data/inputs/chromsizes.pkl'\n",
    "pos_info_path = f'{sequencenpz.split(\"_\")[0]}_pos_info.pkl' # get this path based on sequencenpz path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b271e-09fd-413b-854c-ad5ff215a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drg_tools.io_utils import readinfasta\n",
    "attarrays=outdir+'from'+model_file+'_'+attribution_type+'all.npz'\n",
    "seq_names, seqs = readinfasta(sequence)\n",
    "\n",
    "jpgs = []\n",
    "if plotpermodality:\n",
    "    for modal in modalities:\n",
    "        for s, seq in enumerate(seq_names):\n",
    "            print(seq, modal)\n",
    "            !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} {'musthave='+modal} --chromsizes_path {chromsizes_path} --pos_info_path {pos_info_path} --'save_bw'\n",
    "            jpgs.append(f'{attarrays.rsplit('.',1)[0]}_{seq}_{modal}.jpg')\n",
    "else:\n",
    "    for s, seq in enumerate(seq_names):\n",
    "        print(seq)\n",
    "        !python {drgclis+'sequence_attributions/run_plot_acrosstracks_attribution_maps.py'} {attarrays} {sequencenpz} {seq} 'all' --chromsizes_path {chromsizes_path} --pos_info_path {pos_info_path} --'save_bw'\n",
    "        jpgs.append(f'{attarrays.rsplit('.',1)[0]}_{seq}_all.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f97e1-6ec3-401f-b782-7ecbcfa68420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4b99c44-6021-43d8-9b8d-3dfbbebe21f3",
   "metadata": {},
   "source": [
    "## Extract seqlets from attributions, cluster seqlets, normalize cluster motifs to use with tomtom \n",
    "#### These steps are from https://github.com/LXsasse/DRG/blob/main/examples/Attribution_analysis.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af9182-ea84-46f6-8401-f49384238f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e82ee376-56b3-40a1-8023-87e84eca08b8",
   "metadata": {},
   "source": [
    "#### Extract seqlets from attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e768f8-5340-47c3-bf9f-b60578c0c69c",
   "metadata": {
    "id": "04e768f8-5340-47c3-bf9f-b60578c0c69c"
   },
   "outputs": [],
   "source": [
    "sigcut=1.96 # z-score cut off for significant motifs\n",
    "maxgap=1 # maximum gap length\n",
    "minsize=4 # minimum number of significant bases in a 'motif'\n",
    "norm='global' # Z-score normalization derived from all attributions. Alternatiely, 'seq' if each sequence should be normalized individually, 'condition' if all sequences in a track should be normalized individually, 'std 0.1' if we want to devide by the standard deviation of 0.1 \n",
    "\n",
    "!python {drgclis+'sequence_attributions/run_extract_motifs_from_attributionmaps.py'} {attarrays} {sequencenpz} {sigcut} {maxgap} {minsize} {norm} --select_tracks all\n",
    "\n",
    "seqleteffects=f'{attarrays.split('.npz')[0]}_globalmotifs1.96_1_4.txt'\n",
    "seqlets=f'{attarrays.split('.npz')[0]}_globalmotifs1.96_1_4.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d12c77-37b4-45d3-8ca4-0051f0a459e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7605a56-40b4-4af0-b628-647db39e6bc8",
   "metadata": {},
   "source": [
    "#### Cluster seqlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d114db-cb03-4155-93f5-f2c509de059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {drgclis+'interpret_models/compute_pwm_correlation_cluster_and_combine.py'} {seqlets} complete --distance_threshold 0.05 --distance_metric correlation_pvalue --clusternames\n",
    "\n",
    "seqlet_clusters=f'{os.path.splitext(seqlets)[0]}ms4_cldcomplete0.05corpva.txt'\n",
    "seqlet_clustermotifs=f'{os.path.splitext(seqlets)[0]}ms4_cldcomplete0.05corpvapfms.meme'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d28df-07e2-43d9-93f0-298e6d7447f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f5e0a2-58bf-4fb2-9284-2b23968a3f57",
   "metadata": {},
   "source": [
    "#### Normalize cluster motifs to use with tomtom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31a10b-6a93-4f0b-98a2-76e302c4f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: use --custom_outname to avoid errors based on automatically generated filename being too long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77a6c0-5f92-42ff-8b7f-6d525a36e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip=0 # To avoid 'No entries in pwm' error from parse_motifs_tomeme, set strip=0\n",
    "norm_cluster_outname = 'norm_cluster_motifs'\n",
    "!python {drgclis+'interpret_models/parse_motifs_tomeme.py'} {seqlet_clustermotifs} --transform exp,strip={strip},norm --'custom_outname' {norm_cluster_outname}\n",
    "norm_seqlet_clustermotifs = f'{os.path.dirname(seqlets)}/{norm_cluster_outname}.meme'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafe259-0a3e-4e3e-bd6c-3c58bbe5a977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1fa1a5c-4e2d-46e2-8fa5-f7440f02774c",
   "metadata": {},
   "source": [
    "## Use tomtom to match cluster motif to a database, and save .bed files for the positions of significant motif matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdecb6dd-461c-41f2-a19a-68ad973fa9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the number of .bed files saved will be number of sequences x number of tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba215-851e-4282-a68d-71f8d89093e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_database_path='./data/mouse_pfms_v4.meme'\n",
    "save_dir = './results/' # where the .bed files are saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf478d4-4733-4339-a145-0df647438b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python  {drgclis+'sequence_attributions/save_motif_labels_as_bed.py'} --motif_database_path {motif_database_path} --input_seqlet_path {norm_seqlet_clustermotifs} --motif_locations_info_path {seqlet_clusters} --pos_info_path {pos_info_path} --save_dir {save_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1258118-ab64-45fe-8972-efbf3e237e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfbfc5-ec04-46a0-ae56-86bcd04659c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
